# Common Task 2

## Description

Train a generic next-token-prediction Transformer model to map the input data to the tokenized output sequences. Evaluate performance on the test set using sequence accuracy as a metric.

## Implementation details

I have used x-transformers library to use Encoder-Decoder style transformer, with 2 Encoder blocks and 4 Decoder blocks. After some hyperparameter tuning I have achieved a token_accuracy 95.68% and sequence_accuracy 91.58%.

Model weights can be found here (link)[https://drive.google.com/file/d/12lWfEmwol9ytxnCn4dZmHXelm4A1t6td/view?usp=sharing]

