{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00bde52",
   "metadata": {
    "papermill": {
     "duration": 0.006833,
     "end_time": "2025-04-02T18:19:23.101850",
     "exception": false,
     "start_time": "2025-04-02T18:19:23.095017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb57ab71",
   "metadata": {
    "papermill": {
     "duration": 0.005646,
     "end_time": "2025-04-02T18:19:23.113775",
     "exception": false,
     "start_time": "2025-04-02T18:19:23.108129",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports & Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba2b2733",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T18:19:23.126781Z",
     "iopub.status.busy": "2025-04-02T18:19:23.126496Z",
     "iopub.status.idle": "2025-04-02T18:19:27.189865Z",
     "shell.execute_reply": "2025-04-02T18:19:27.189130Z"
    },
    "papermill": {
     "duration": 4.071773,
     "end_time": "2025-04-02T18:19:27.191459",
     "exception": false,
     "start_time": "2025-04-02T18:19:23.119686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31cf9a1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T18:19:27.205386Z",
     "iopub.status.busy": "2025-04-02T18:19:27.204969Z",
     "iopub.status.idle": "2025-04-02T18:19:32.465770Z",
     "shell.execute_reply": "2025-04-02T18:19:32.464919Z"
    },
    "papermill": {
     "duration": 5.269211,
     "end_time": "2025-04-02T18:19:32.467420",
     "exception": false,
     "start_time": "2025-04-02T18:19:27.198209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.2/88.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.0/103.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q x-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c17c5fc",
   "metadata": {
    "papermill": {
     "duration": 0.006714,
     "end_time": "2025-04-02T18:19:32.481208",
     "exception": false,
     "start_time": "2025-04-02T18:19:32.474494",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1e9a247",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T18:19:32.494698Z",
     "iopub.status.busy": "2025-04-02T18:19:32.494440Z",
     "iopub.status.idle": "2025-04-02T18:19:34.255403Z",
     "shell.execute_reply": "2025-04-02T18:19:34.254460Z"
    },
    "papermill": {
     "duration": 1.769465,
     "end_time": "2025-04-02T18:19:34.256987",
     "exception": false,
     "start_time": "2025-04-02T18:19:32.487522",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 12441, Validation size: 1555, Test size: 1556\n"
     ]
    }
   ],
   "source": [
    "def read_txt(path):\n",
    "    with open(path,'r', encoding='utf-8') as f:\n",
    "        lines = f.read().split('\\n')\n",
    "    text_pairs = []\n",
    "    mx = 0 \n",
    "    mn = 100\n",
    "    x = []\n",
    "    for _line in lines:\n",
    "        arr = _line.split(':')\n",
    "        if len(arr) == 1:\n",
    "            continue\n",
    "        text_pairs.append({'Event Type': arr[0], \\\n",
    "                          'Feynman Diagram': arr[1],   \\\n",
    "                           'Amplitude': arr[-2],        \\\n",
    "                           'Squared Amplitude': arr[-1] \\\n",
    "                          })\n",
    "    return text_pairs\n",
    "\n",
    "final_pairs = [read_txt(\n",
    "            f'/kaggle/input/squared-amplitudes-test-data/SYMBA - Test Data/QED-2-to-2-diag-TreeLevel-{i}.txt')\n",
    "            for i in range(10)]\n",
    "final_pairs = [xx for x in final_pairs for xx in x]\n",
    "\n",
    "# df = pd.DataFrame(final_pairs,columns=['Event Type','Feynman Diagram', 'Amplitude', 'Squared Amplitude'],dtype=['str','str','str','str'])\n",
    "df = pd.DataFrame(final_pairs, columns=['Event Type', 'Feynman Diagram', 'Amplitude', 'Squared Amplitude'])\n",
    "df = df.astype({'Event Type': 'string', 'Feynman Diagram': 'string', 'Amplitude': 'string', 'Squared Amplitude': 'string'})\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_df)}\")\n",
    "\n",
    "train_df.to_csv('train.csv')\n",
    "val_df.to_csv('val.csv')\n",
    "test_df.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8afe540",
   "metadata": {
    "papermill": {
     "duration": 0.006642,
     "end_time": "2025-04-02T18:19:34.270765",
     "exception": false,
     "start_time": "2025-04-02T18:19:34.264123",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Vocab Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7190eff9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T18:19:34.284474Z",
     "iopub.status.busy": "2025-04-02T18:19:34.284171Z",
     "iopub.status.idle": "2025-04-02T18:19:34.288529Z",
     "shell.execute_reply": "2025-04-02T18:19:34.287859Z"
    },
    "papermill": {
     "duration": 0.012543,
     "end_time": "2025-04-02T18:19:34.289759",
     "exception": false,
     "start_time": "2025-04-02T18:19:34.277216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reporthook(t):\n",
    "    \"\"\"\n",
    "    https://github.com/tqdm/tqdm.\n",
    "    \"\"\"\n",
    "    last_b = [0]\n",
    "\n",
    "    def inner(b=1, bsize=1, tsize=None):\n",
    "        \"\"\"\n",
    "        b: int, optional\n",
    "        Number of blocks just transferred [default: 1].\n",
    "        bsize: int, optional\n",
    "        Size of each block (in tqdm units) [default: 1].\n",
    "        tsize: int, optional\n",
    "        Total size (in tqdm units). If [default: None] remains unchanged.\n",
    "        \"\"\"\n",
    "        if tsize is not None:\n",
    "            t.total = tsize\n",
    "        t.update((b - last_b[0]) * bsize)\n",
    "        last_b[0] = b\n",
    "\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3428e389",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T18:19:34.303647Z",
     "iopub.status.busy": "2025-04-02T18:19:34.303431Z",
     "iopub.status.idle": "2025-04-02T18:19:34.334086Z",
     "shell.execute_reply": "2025-04-02T18:19:34.333461Z"
    },
    "papermill": {
     "duration": 0.039159,
     "end_time": "2025-04-02T18:19:34.335262",
     "exception": false,
     "start_time": "2025-04-02T18:19:34.296103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mapping from string name to factory function'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import logging\n",
    "import os\n",
    "import zipfile\n",
    "import gzip\n",
    "from urllib.request import urlretrieve\n",
    "from tqdm import tqdm\n",
    "import tarfile\n",
    "from functools import partial\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def _infer_shape(f):\n",
    "    num_lines, vector_dim = 0, None\n",
    "    for line in f:\n",
    "        if vector_dim is None:\n",
    "            row = line.rstrip().split(b\" \")\n",
    "            vector = row[1:]\n",
    "            # Assuming word, [vector] format\n",
    "            if len(vector) > 2:\n",
    "                # The header present in some (w2v) formats contains two elements.\n",
    "                vector_dim = len(vector)\n",
    "                num_lines += 1  # First element read\n",
    "        else:\n",
    "            num_lines += 1\n",
    "    f.seek(0)\n",
    "    return num_lines, vector_dim\n",
    "\n",
    "\n",
    "class Vectors(object):\n",
    "\n",
    "    def __init__(self, name, cache=None,\n",
    "                 url=None, unk_init=None, max_vectors=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "\n",
    "            name: name of the file that contains the vectors\n",
    "            cache: directory for cached vectors\n",
    "            url: url for download if vectors not found in cache\n",
    "            unk_init (callback): by default, initialize out-of-vocabulary word vectors\n",
    "                to zero vectors; can be any function that takes in a Tensor and returns a Tensor of the same size\n",
    "            max_vectors (int): this can be used to limit the number of\n",
    "                pre-trained vectors loaded.\n",
    "                Most pre-trained vector sets are sorted\n",
    "                in the descending order of word frequency.\n",
    "                Thus, in situations where the entire set doesn't fit in memory,\n",
    "                or is not needed for another reason, passing `max_vectors`\n",
    "                can limit the size of the loaded set.\n",
    "        \"\"\"\n",
    "\n",
    "        cache = '.vector_cache' if cache is None else cache\n",
    "        self.itos = None\n",
    "        self.stoi = None\n",
    "        self.vectors = None\n",
    "        self.dim = None\n",
    "        self.unk_init = torch.Tensor.zero_ if unk_init is None else unk_init\n",
    "        self.cache(name, cache, url=url, max_vectors=max_vectors)\n",
    "\n",
    "    def __getitem__(self, token):\n",
    "        if token in self.stoi:\n",
    "            return self.vectors[self.stoi[token]]\n",
    "        else:\n",
    "            return self.unk_init(torch.Tensor(self.dim))\n",
    "\n",
    "    def cache(self, name, cache, url=None, max_vectors=None):\n",
    "        import ssl\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "        if os.path.isfile(name):\n",
    "            path = name\n",
    "            if max_vectors:\n",
    "                file_suffix = '_{}.pt'.format(max_vectors)\n",
    "            else:\n",
    "                file_suffix = '.pt'\n",
    "            path_pt = os.path.join(cache, os.path.basename(name)) + file_suffix\n",
    "        else:\n",
    "            path = os.path.join(cache, name)\n",
    "            if max_vectors:\n",
    "                file_suffix = '_{}.pt'.format(max_vectors)\n",
    "            else:\n",
    "                file_suffix = '.pt'\n",
    "            path_pt = path + file_suffix\n",
    "\n",
    "        if not os.path.isfile(path_pt):\n",
    "            if not os.path.isfile(path) and url:\n",
    "                logger.info('Downloading vectors from {}'.format(url))\n",
    "                if not os.path.exists(cache):\n",
    "                    os.makedirs(cache)\n",
    "                dest = os.path.join(cache, os.path.basename(url))\n",
    "                if not os.path.isfile(dest):\n",
    "                    with tqdm(unit='B', unit_scale=True, miniters=1, desc=dest) as t:\n",
    "                        try:\n",
    "                            urlretrieve(url, dest, reporthook=reporthook(t))\n",
    "                        except KeyboardInterrupt as e:  # remove the partial zip file\n",
    "                            os.remove(dest)\n",
    "                            raise e\n",
    "                logger.info('Extracting vectors into {}'.format(cache))\n",
    "                ext = os.path.splitext(dest)[1][1:]\n",
    "                if ext == 'zip':\n",
    "                    with zipfile.ZipFile(dest, \"r\") as zf:\n",
    "                        zf.extractall(cache)\n",
    "                elif ext == 'gz':\n",
    "                    if dest.endswith('.tar.gz'):\n",
    "                        with tarfile.open(dest, 'r:gz') as tar:\n",
    "                            tar.extractall(path=cache)\n",
    "            if not os.path.isfile(path):\n",
    "                raise RuntimeError('no vectors found at {}'.format(path))\n",
    "\n",
    "            logger.info(\"Loading vectors from {}\".format(path))\n",
    "            ext = os.path.splitext(path)[1][1:]\n",
    "            if ext == 'gz':\n",
    "                open_file = gzip.open\n",
    "            else:\n",
    "                open_file = open\n",
    "\n",
    "            vectors_loaded = 0\n",
    "            with open_file(path, 'rb') as f:\n",
    "                num_lines, dim = _infer_shape(f)\n",
    "                if not max_vectors or max_vectors > num_lines:\n",
    "                    max_vectors = num_lines\n",
    "\n",
    "                itos, vectors, dim = [], torch.zeros((max_vectors, dim)), None\n",
    "\n",
    "                for line in tqdm(f, total=max_vectors):\n",
    "                    # Explicitly splitting on \" \" is important, so we don't\n",
    "                    # get rid of Unicode non-breaking spaces in the vectors.\n",
    "                    entries = line.rstrip().split(b\" \")\n",
    "\n",
    "                    word, entries = entries[0], entries[1:]\n",
    "                    if dim is None and len(entries) > 1:\n",
    "                        dim = len(entries)\n",
    "                    elif len(entries) == 1:\n",
    "                        logger.warning(\"Skipping token {} with 1-dimensional \"\n",
    "                                       \"vector {}; likely a header\".format(word, entries))\n",
    "                        continue\n",
    "                    elif dim != len(entries):\n",
    "                        raise RuntimeError(\n",
    "                            \"Vector for token {} has {} dimensions, but previously \"\n",
    "                            \"read vectors have {} dimensions. All vectors must have \"\n",
    "                            \"the same number of dimensions.\".format(word, len(entries),\n",
    "                                                                    dim))\n",
    "\n",
    "                    try:\n",
    "                        if isinstance(word, bytes):\n",
    "                            word = word.decode('utf-8')\n",
    "                    except UnicodeDecodeError:\n",
    "                        logger.info(\"Skipping non-UTF8 token {}\".format(repr(word)))\n",
    "                        continue\n",
    "\n",
    "                    vectors[vectors_loaded] = torch.tensor([float(x) for x in entries])\n",
    "                    vectors_loaded += 1\n",
    "                    itos.append(word)\n",
    "\n",
    "                    if vectors_loaded == max_vectors:\n",
    "                        break\n",
    "\n",
    "            self.itos = itos\n",
    "            self.stoi = {word: i for i, word in enumerate(itos)}\n",
    "            self.vectors = torch.Tensor(vectors).view(-1, dim)\n",
    "            self.dim = dim\n",
    "            logger.info('Saving vectors to {}'.format(path_pt))\n",
    "            if not os.path.exists(cache):\n",
    "                os.makedirs(cache)\n",
    "            torch.save((self.itos, self.stoi, self.vectors, self.dim), path_pt)\n",
    "        else:\n",
    "            logger.info('Loading vectors from {}'.format(path_pt))\n",
    "            self.itos, self.stoi, self.vectors, self.dim = torch.load(path_pt)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vectors)\n",
    "\n",
    "    def get_vecs_by_tokens(self, tokens, lower_case_backup=False):\n",
    "        \"\"\"Look up embedding vectors of tokens.\n",
    "\n",
    "        Args:\n",
    "            tokens: a token or a list of tokens. if `tokens` is a string,\n",
    "                returns a 1-D tensor of shape `self.dim`; if `tokens` is a\n",
    "                list of strings, returns a 2-D tensor of shape=(len(tokens),\n",
    "                self.dim).\n",
    "            lower_case_backup : Whether to look up the token in the lower case.\n",
    "                If False, each token in the original case will be looked up;\n",
    "                if True, each token in the original case will be looked up first,\n",
    "                if not found in the keys of the property `stoi`, the token in the\n",
    "                lower case will be looked up. Default: False.\n",
    "\n",
    "        Examples:\n",
    "            >>> examples = ['chip', 'baby', 'Beautiful']\n",
    "            >>> vec = text.vocab.GloVe(name='6B', dim=50)\n",
    "            >>> ret = vec.get_vecs_by_tokens(tokens, lower_case_backup=True)\n",
    "        \"\"\"\n",
    "        to_reduce = False\n",
    "\n",
    "        if not isinstance(tokens, list):\n",
    "            tokens = [tokens]\n",
    "            to_reduce = True\n",
    "\n",
    "        if not lower_case_backup:\n",
    "            indices = [self[token] for token in tokens]\n",
    "        else:\n",
    "            indices = [self[token] if token in self.stoi\n",
    "                       else self[token.lower()]\n",
    "                       for token in tokens]\n",
    "\n",
    "        vecs = torch.stack(indices)\n",
    "        return vecs[0] if to_reduce else vecs\n",
    "\n",
    "\n",
    "class GloVe(Vectors):\n",
    "    url = {\n",
    "        '42B': 'http://nlp.stanford.edu/data/glove.42B.300d.zip',\n",
    "        '840B': 'http://nlp.stanford.edu/data/glove.840B.300d.zip',\n",
    "        'twitter.27B': 'http://nlp.stanford.edu/data/glove.twitter.27B.zip',\n",
    "        '6B': 'http://nlp.stanford.edu/data/glove.6B.zip',\n",
    "    }\n",
    "\n",
    "    def __init__(self, name='840B', dim=300, **kwargs):\n",
    "        url = self.url[name]\n",
    "        name = 'glove.{}.{}d.txt'.format(name, str(dim))\n",
    "        super(GloVe, self).__init__(name, url=url, **kwargs)\n",
    "\n",
    "\n",
    "class FastText(Vectors):\n",
    "\n",
    "    url_base = 'https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.{}.vec'\n",
    "\n",
    "    def __init__(self, language=\"en\", **kwargs):\n",
    "        url = self.url_base.format(language)\n",
    "        name = os.path.basename(url)\n",
    "        super(FastText, self).__init__(name, url=url, **kwargs)\n",
    "\n",
    "\n",
    "class CharNGram(Vectors):\n",
    "\n",
    "    name = 'charNgram.txt'\n",
    "    url = ('http://www.logos.t.u-tokyo.ac.jp/~hassy/publications/arxiv2016jmt/'\n",
    "           'jmt_pre-trained_embeddings.tar.gz')\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CharNGram, self).__init__(self.name, url=self.url, **kwargs)\n",
    "\n",
    "    def __getitem__(self, token):\n",
    "        vector = torch.Tensor(1, self.dim).zero_()\n",
    "        if token == \"<unk>\":\n",
    "            return self.unk_init(vector)\n",
    "        chars = ['#BEGIN#'] + list(token) + ['#END#']\n",
    "        num_vectors = 0\n",
    "        for n in [2, 3, 4]:\n",
    "            end = len(chars) - n + 1\n",
    "            grams = [chars[i:(i + n)] for i in range(end)]\n",
    "            for gram in grams:\n",
    "                gram_key = '{}gram-{}'.format(n, ''.join(gram))\n",
    "                if gram_key in self.stoi:\n",
    "                    vector += self.vectors[self.stoi[gram_key]]\n",
    "                    num_vectors += 1\n",
    "        if num_vectors > 0:\n",
    "            vector /= num_vectors\n",
    "        else:\n",
    "            vector = self.unk_init(vector)\n",
    "        return vector\n",
    "\n",
    "\n",
    "pretrained_aliases = {\n",
    "    \"charngram.100d\": partial(CharNGram),\n",
    "    \"fasttext.en.300d\": partial(FastText, language=\"en\"),\n",
    "    \"fasttext.simple.300d\": partial(FastText, language=\"simple\"),\n",
    "    \"glove.42B.300d\": partial(GloVe, name=\"42B\", dim=\"300\"),\n",
    "    \"glove.840B.300d\": partial(GloVe, name=\"840B\", dim=\"300\"),\n",
    "    \"glove.twitter.27B.25d\": partial(GloVe, name=\"twitter.27B\", dim=\"25\"),\n",
    "    \"glove.twitter.27B.50d\": partial(GloVe, name=\"twitter.27B\", dim=\"50\"),\n",
    "    \"glove.twitter.27B.100d\": partial(GloVe, name=\"twitter.27B\", dim=\"100\"),\n",
    "    \"glove.twitter.27B.200d\": partial(GloVe, name=\"twitter.27B\", dim=\"200\"),\n",
    "    \"glove.6B.50d\": partial(GloVe, name=\"6B\", dim=\"50\"),\n",
    "    \"glove.6B.100d\": partial(GloVe, name=\"6B\", dim=\"100\"),\n",
    "    \"glove.6B.200d\": partial(GloVe, name=\"6B\", dim=\"200\"),\n",
    "    \"glove.6B.300d\": partial(GloVe, name=\"6B\", dim=\"300\")\n",
    "}\n",
    "\"\"\"Mapping from string name to factory function\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9016906",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T18:19:34.348914Z",
     "iopub.status.busy": "2025-04-02T18:19:34.348705Z",
     "iopub.status.idle": "2025-04-02T18:19:34.368025Z",
     "shell.execute_reply": "2025-04-02T18:19:34.367203Z"
    },
    "papermill": {
     "duration": 0.027564,
     "end_time": "2025-04-02T18:19:34.369233",
     "exception": false,
     "start_time": "2025-04-02T18:19:34.341669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import logging\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "# from torchtext.vocab import (\n",
    "#     pretrained_aliases,  # not in legacy\n",
    "#     Vectors,  # not in legacy\n",
    "# )\n",
    "from typing import List\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Vocab(object):\n",
    "    \"\"\"Defines a vocabulary object that will be used to numericalize a field.\n",
    "\n",
    "    Attributes:\n",
    "        freqs: A collections.Counter object holding the frequencies of tokens\n",
    "            in the data used to build the Vocab.\n",
    "        stoi: A collections.defaultdict instance mapping token strings to\n",
    "            numerical identifiers.\n",
    "        itos: A list of token strings indexed by their numerical identifiers.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO (@mttk): Populate classs with default values of special symbols\n",
    "    UNK = '<unk>'\n",
    "\n",
    "    def __init__(self, counter, max_size=None, min_freq=1, specials=('<unk>', '<pad>'),\n",
    "                 vectors=None, unk_init=None, vectors_cache=None, specials_first=True):\n",
    "        \"\"\"Create a Vocab object from a collections.Counter.\n",
    "\n",
    "        Args:\n",
    "            counter: collections.Counter object holding the frequencies of\n",
    "                each value found in the data.\n",
    "            max_size: The maximum size of the vocabulary, or None for no\n",
    "                maximum. Default: None.\n",
    "            min_freq: The minimum frequency needed to include a token in the\n",
    "                vocabulary. Values less than 1 will be set to 1. Default: 1.\n",
    "            specials: The list of special tokens (e.g., padding or eos) that\n",
    "                will be prepended to the vocabulary. Default: ['<unk'>, '<pad>']\n",
    "            vectors: One of either the available pretrained vectors\n",
    "                or custom pretrained vectors (see Vocab.load_vectors);\n",
    "                or a list of aforementioned vectors\n",
    "            unk_init (callback): by default, initialize out-of-vocabulary word vectors\n",
    "                to zero vectors; can be any function that takes in a Tensor and\n",
    "                returns a Tensor of the same size. Default: 'torch.zeros'\n",
    "            vectors_cache: directory for cached vectors. Default: '.vector_cache'\n",
    "            specials_first: Whether to add special tokens into the vocabulary at first.\n",
    "                If it is False, they are added into the vocabulary at last.\n",
    "                Default: True.\n",
    "        \"\"\"\n",
    "        self.freqs = counter\n",
    "        counter = counter.copy()\n",
    "        min_freq = max(min_freq, 1)\n",
    "\n",
    "        self.itos = list()\n",
    "        self.unk_index = None\n",
    "        if specials_first:\n",
    "            self.itos = list(specials)\n",
    "            # only extend max size if specials are prepended\n",
    "            max_size = None if max_size is None else max_size + len(specials)\n",
    "\n",
    "        # frequencies of special tokens are not counted when building vocabulary\n",
    "        # in frequency order\n",
    "        for tok in specials:\n",
    "            if tok in counter:\n",
    "                del counter[tok]\n",
    "\n",
    "        # sort by frequency, then alphabetically\n",
    "        words_and_frequencies = sorted(counter.items(), key=lambda tup: tup[0])\n",
    "        words_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "        for word, freq in words_and_frequencies:\n",
    "            if freq < min_freq or len(self.itos) == max_size:\n",
    "                break\n",
    "            self.itos.append(word)\n",
    "\n",
    "        if Vocab.UNK in specials:  # hard-coded for now\n",
    "            unk_index = specials.index(Vocab.UNK)  # position in list\n",
    "            # account for ordering of specials, set variable\n",
    "            self.unk_index = unk_index if specials_first else len(self.itos) + unk_index\n",
    "            self.stoi = defaultdict(self._default_unk_index)\n",
    "        else:\n",
    "            self.stoi = defaultdict()\n",
    "\n",
    "        if not specials_first:\n",
    "            self.itos.extend(list(specials))\n",
    "\n",
    "        # stoi is simply a reverse dict for itos\n",
    "        self.stoi.update({tok: i for i, tok in enumerate(self.itos)})\n",
    "\n",
    "        self.vectors = None\n",
    "        if vectors is not None:\n",
    "            self.load_vectors(vectors, unk_init=unk_init, cache=vectors_cache)\n",
    "        else:\n",
    "            assert unk_init is None and vectors_cache is None\n",
    "\n",
    "    def forward(self, tokens: List[str]) -> List[int]:\n",
    "        return self.lookup_indices(tokens)\n",
    "\n",
    "    def set_default_index(self, idx):\n",
    "        self.unk_index = idx\n",
    "        \n",
    "    def _default_unk_index(self):\n",
    "        return self.unk_index\n",
    "\n",
    "    def __getitem__(self, token):\n",
    "        return self.stoi.get(token, self.stoi.get(Vocab.UNK))\n",
    "\n",
    "    def __getstate__(self):\n",
    "        # avoid picking defaultdict\n",
    "        attrs = dict(self.__dict__)\n",
    "        # cast to regular dict\n",
    "        attrs['stoi'] = dict(self.stoi)\n",
    "        return attrs\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if state.get(\"unk_index\", None) is None:\n",
    "            stoi = defaultdict()\n",
    "        else:\n",
    "            stoi = defaultdict(self._default_unk_index)\n",
    "        stoi.update(state['stoi'])\n",
    "        state['stoi'] = stoi\n",
    "        self.__dict__.update(state)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if self.freqs != other.freqs:\n",
    "            return False\n",
    "        if self.stoi != other.stoi:\n",
    "            return False\n",
    "        if self.itos != other.itos:\n",
    "            return False\n",
    "        if self.vectors != other.vectors:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def lookup_indices(self, tokens):\n",
    "        indices = [self.__getitem__(token) for token in tokens]\n",
    "        return indices\n",
    "\n",
    "    def extend(self, v, sort=False):\n",
    "        words = sorted(v.itos) if sort else v.itos\n",
    "        for w in words:\n",
    "            if w not in self.stoi:\n",
    "                self.itos.append(w)\n",
    "                self.stoi[w] = len(self.itos) - 1\n",
    "\n",
    "    def load_vectors(self, vectors, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vectors: one of or a list containing instantiations of the\n",
    "                GloVe, CharNGram, or Vectors classes. Alternatively, one\n",
    "                of or a list of available pretrained vectors:\n",
    "\n",
    "                charngram.100d\n",
    "                fasttext.en.300d\n",
    "                fasttext.simple.300d\n",
    "                glove.42B.300d\n",
    "                glove.840B.300d\n",
    "                glove.twitter.27B.25d\n",
    "                glove.twitter.27B.50d\n",
    "                glove.twitter.27B.100d\n",
    "                glove.twitter.27B.200d\n",
    "                glove.6B.50d\n",
    "                glove.6B.100d\n",
    "                glove.6B.200d\n",
    "                glove.6B.300d\n",
    "\n",
    "            Remaining keyword arguments: Passed to the constructor of Vectors classes.\n",
    "        \"\"\"\n",
    "        if not isinstance(vectors, list):\n",
    "            vectors = [vectors]\n",
    "        for idx, vector in enumerate(vectors):\n",
    "            if isinstance(vector, str):\n",
    "                # Convert the string pretrained vector identifier\n",
    "                # to a Vectors object\n",
    "                if vector not in pretrained_aliases:\n",
    "                    raise ValueError(\"Got string input vector {}, but allowed pretrained vectors are {}\".format(vector, list(pretrained_aliases.keys())))\n",
    "                vectors[idx] = pretrained_aliases[vector](**kwargs)\n",
    "            elif not isinstance(vector, Vectors):\n",
    "                raise ValueError( \"Got input vectors of type {}, expected str or Vectors object\".format(type(vector)))\n",
    "        tot_dim = sum(v.dim for v in vectors)\n",
    "        self.vectors = torch.Tensor(len(self), tot_dim)\n",
    "        for i, token in enumerate(self.itos):\n",
    "            start_dim = 0\n",
    "            for v in vectors:\n",
    "                end_dim = start_dim + v.dim\n",
    "                self.vectors[i][start_dim:end_dim] = v[token.strip()]\n",
    "                start_dim = end_dim\n",
    "            assert(start_dim == tot_dim)\n",
    "\n",
    "    def set_vectors(self, stoi, vectors, dim, unk_init=torch.Tensor.zero_):\n",
    "        \"\"\"\n",
    "        Set the vectors for the Vocab instance from a collection of Tensors.\n",
    "\n",
    "        Args:\n",
    "            stoi: A dictionary of string to the index of the associated vector\n",
    "                in the `vectors` input argument.\n",
    "            vectors: An indexed iterable (or other structure supporting __getitem__) that\n",
    "                given an input index, returns a FloatTensor representing the vector\n",
    "                for the token associated with the index. For example,\n",
    "                vector[stoi[\"string\"]] should return the vector for \"string\".\n",
    "            dim: The dimensionality of the vectors.\n",
    "            unk_init (callback): by default, initialize out-of-vocabulary word vectors\n",
    "                to zero vectors; can be any function that takes in a Tensor and\n",
    "                returns a Tensor of the same size. Default: 'torch.zeros'\n",
    "        \"\"\"\n",
    "        self.vectors = torch.Tensor(len(self), dim)\n",
    "        for i, token in enumerate(self.itos):\n",
    "            wv_index = stoi.get(token, None)\n",
    "            if wv_index is not None:\n",
    "                self.vectors[i] = vectors[wv_index]\n",
    "            else:\n",
    "                self.vectors[i] = unk_init(self.vectors[i])\n",
    "\n",
    "\n",
    "class SubwordVocab(Vocab):\n",
    "\n",
    "    def __init__(self, counter, max_size=None, specials=('<pad>'),\n",
    "                 vectors=None, unk_init=torch.Tensor.zero_):\n",
    "        \"\"\"Create a revtok subword vocabulary from a collections.Counter.\n",
    "\n",
    "        Args:\n",
    "            counter: collections.Counter object holding the frequencies of\n",
    "                each word found in the data.\n",
    "            max_size: The maximum size of the subword vocabulary, or None for no\n",
    "                maximum. Default: None.\n",
    "            specials: The list of special tokens (e.g., padding or eos) that\n",
    "                will be prepended to the vocabulary in addition to an <unk>\n",
    "                token.\n",
    "            vectors: One of either the available pretrained vectors\n",
    "                or custom pretrained vectors (see Vocab.load_vectors);\n",
    "                or a list of aforementioned vectors\n",
    "            unk_init (callback): by default, initialize out-of-vocabulary word vectors\n",
    "                to zero vectors; can be any function that takes in a Tensor and\n",
    "                returns a Tensor of the same size. Default: 'torch.zeros\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import revtok\n",
    "        except ImportError:\n",
    "            print(\"Please install revtok.\")\n",
    "            raise\n",
    "\n",
    "        # Hardcode unk_index as subword_vocab has no specials_first argument\n",
    "        self.unk_index = (specials.index(SubwordVocab.UNK)\n",
    "                          if SubwordVocab.UNK in specials else None)\n",
    "\n",
    "        if self.unk_index is None:\n",
    "            self.stoi = defaultdict()\n",
    "        else:\n",
    "            self.stoi = defaultdict(self._default_unk_index)\n",
    "\n",
    "        self.stoi.update({tok: i for i, tok in enumerate(specials)})\n",
    "        self.itos = specials.copy()\n",
    "\n",
    "        self.segment = revtok.SubwordSegmenter(counter, max_size)\n",
    "\n",
    "        max_size = None if max_size is None else max_size + len(self.itos)\n",
    "\n",
    "        # sort by frequency/entropy, then alphabetically\n",
    "        toks = sorted(self.segment.vocab.items(), key=lambda tup: (len(tup[0]) != 1, -tup[1], tup[0]))\n",
    "\n",
    "        for tok, _ in toks:\n",
    "            if len(self.itos) == max_size:\n",
    "                break\n",
    "            self.itos.append(tok)\n",
    "            self.stoi[tok] = len(self.itos) - 1\n",
    "\n",
    "        if vectors is not None:\n",
    "            self.load_vectors(vectors, unk_init=unk_init)\n",
    "\n",
    "\n",
    "def build_vocab_from_iterator(iterator, num_lines=None):\n",
    "    \"\"\"\n",
    "    Build a Vocab from an iterator.\n",
    "\n",
    "    Args:\n",
    "        iterator: Iterator used to build Vocab. Must yield list or iterator of tokens.\n",
    "        num_lines: The expected number of elements returned by the iterator.\n",
    "            (Default: None)\n",
    "            Optionally, if known, the expected number of elements can be passed to\n",
    "            this factory function for improved progress reporting.\n",
    "    \"\"\"\n",
    "\n",
    "    counter = Counter()\n",
    "    with tqdm(unit_scale=0, unit='lines', total=num_lines) as t:\n",
    "        for tokens in iterator:\n",
    "            counter.update(tokens)\n",
    "            t.update(1)\n",
    "    word_vocab = Vocab(counter)\n",
    "    return word_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7b5e09",
   "metadata": {
    "papermill": {
     "duration": 0.006187,
     "end_time": "2025-04-02T18:19:34.382035",
     "exception": false,
     "start_time": "2025-04-02T18:19:34.375848",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26c3466a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T18:19:34.395582Z",
     "iopub.status.busy": "2025-04-02T18:19:34.395323Z",
     "iopub.status.idle": "2025-04-02T18:19:34.408927Z",
     "shell.execute_reply": "2025-04-02T18:19:34.408147Z"
    },
    "papermill": {
     "duration": 0.02186,
     "end_time": "2025-04-02T18:19:34.410203",
     "exception": false,
     "start_time": "2025-04-02T18:19:34.388343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "from itertools import cycle\n",
    "import re\n",
    "import random\n",
    "# from torchtext.vocab import vocab # Built it custom\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Tokenizer for processing symbolic mathematical expressions.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, index_token_pool_size, momentum_token_pool_size, special_symbols, UNK_IDX, to_replace):\n",
    "        self.amps = df.Amplitude.tolist()\n",
    "        self.sqamps = df['Squared Amplitude'].tolist()\n",
    "\n",
    "        # Issue warnings if token pool sizes are too small\n",
    "        if index_token_pool_size < 100:\n",
    "            warnings.warn(f\"Index token pool size ({index_token_pool_size}) is small. Consider increasing it.\", UserWarning)\n",
    "        if momentum_token_pool_size < 100:\n",
    "            warnings.warn(f\"Momentum token pool size ({momentum_token_pool_size}) is small. Consider increasing it.\", UserWarning)\n",
    "        \n",
    "        # Generate token pools\n",
    "        self.tokens_pool = [f\"_{i}\" for i in range(index_token_pool_size)]\n",
    "        self.momentum_pool = [f\"MOMENTUM_{i}\" for i in range(momentum_token_pool_size)]\n",
    "        \n",
    "        # Regular expression patterns for token replacement\n",
    "        self.pattern_momentum = re.compile(r'\\b[p]_\\d{1,}\\b')\n",
    "        self.pattern_num_123 = re.compile(r'\\b(?![ps]_)\\w+_\\d{1,}\\b')\n",
    "        self.pattern_special = re.compile(r'\\b\\w+_+\\w+\\b\\\\')\n",
    "        self.pattern_prop = re.compile(r'Prop')\n",
    "        self.pattern_int = re.compile(r'int\\{')\n",
    "        self.pattern_operators = {\n",
    "            '+': re.compile(r'\\+'), '-': re.compile(r'-'), '*': re.compile(r'\\*'),\n",
    "            ',': re.compile(r','), '^': re.compile(r'\\^'), '%': re.compile(r'%'),\n",
    "            '}': re.compile(r'\\}'), '(': re.compile(r'\\('), ')': re.compile(r'\\)')\n",
    "        }\n",
    "        self.function_opening = re.compile(r'(\\w+_\\{)')\n",
    "        self.pattern_mass = re.compile(r'\\b\\w+_\\w\\b')\n",
    "        self.pattern_s = re.compile(r'\\b\\w+_\\d{2,}\\b')\n",
    "        self.pattern_reg_prop = re.compile(r'\\b\\w+_\\d{1}\\b')\n",
    "        self.pattern_antipart = re.compile(r'(\\w)_\\w+_\\d+\\(X\\)\\^\\(\\*\\)')\n",
    "        self.pattern_part = re.compile(r'(\\w)_\\w+_\\d+\\(X\\)')\n",
    "        self.pattern_index = re.compile(r'\\b\\w+_\\w+_\\d{2,}\\b')\n",
    "        \n",
    "        self.special_symbols = special_symbols\n",
    "        self.UNK_IDX = UNK_IDX\n",
    "        self.to_replace = to_replace\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_whitespace(expression):\n",
    "        \"\"\"Remove all forms of whitespace from the expression.\"\"\"\n",
    "        return re.sub(r'\\s+', '', expression)\n",
    "\n",
    "    @staticmethod\n",
    "    def split_expression(expression):\n",
    "        \"\"\"Split the expression by space delimiter.\"\"\"\n",
    "        return re.split(r' ', expression)\n",
    "\n",
    "    def build_tgt_vocab(self):\n",
    "        \"\"\"Build vocabulary for target sequences.\"\"\"\n",
    "        counter = Counter()\n",
    "        for eqn in tqdm(self.sqamps, desc='Processing target vocab'):\n",
    "            counter.update(self.tgt_tokenize(eqn))\n",
    "        voc = Vocab(OrderedDict(counter), specials=self.special_symbols[:], specials_first=True)\n",
    "        voc.set_default_index(self.UNK_IDX)\n",
    "        return voc\n",
    "\n",
    "    def build_src_vocab(self, seed):\n",
    "        \"\"\"Build vocabulary for source sequences.\"\"\"\n",
    "        counter = Counter()\n",
    "        for diag in tqdm(self.amps, desc='Processing source vocab'):\n",
    "            counter.update(self.src_tokenize(diag, seed))\n",
    "        voc = Vocab(OrderedDict(counter), specials=self.special_symbols[:], specials_first=True)\n",
    "        voc.set_default_index(self.UNK_IDX)\n",
    "        return voc\n",
    "    \n",
    "    def src_replace(self, ampl, seed):\n",
    "        \"\"\"Replace indexed and momentum variables with tokenized equivalents.\"\"\"\n",
    "        ampl = self.remove_whitespace(ampl)\n",
    "        \n",
    "        random.seed(seed)\n",
    "        token_cycle = cycle(random.sample(self.tokens_pool, len(self.tokens_pool)))\n",
    "        momentum_cycle = cycle(random.sample(self.momentum_pool, len(self.momentum_pool)))\n",
    "        \n",
    "        # Replace momentum tokens\n",
    "        temp_ampl = ampl\n",
    "        momentum_mapping = {match: next(momentum_cycle) for match in set(self.pattern_momentum.findall(ampl))}\n",
    "        for key, value in momentum_mapping.items():\n",
    "            temp_ampl = temp_ampl.replace(key, value)\n",
    "\n",
    "\n",
    "        def replace_123_match(match):\n",
    "            word, num = match.group().rsplit('_', 1)\n",
    "            # if word == 's':\n",
    "            #     # Mandstein\n",
    "            #     return match.group()\n",
    "            # if word == 'p':\n",
    "            #     # return match.group()  # Keep 'p_X' unchanged\n",
    "            #     return f'MOMENTUM_{num}'\n",
    "            # IDX_POOL.setdefault(num, len(IDX_POOL) + 1)\n",
    "            return f\"{word}_INDEX_{next(token_cycle)}\"\n",
    "        # Replace index tokens\n",
    "        # num_123_mapping = {match: match.rsplit('_',1) + next(token_cycle) for match in set(self.pattern_num_123.findall(ampl))}\n",
    "        # for key, value in num_123_mapping.items():\n",
    "        #     temp_ampl = temp_ampl.replace(key, value)\n",
    "        temp_ampl = re.sub(self.pattern_num_123,replace_123_match,ampl)\n",
    "        return temp_ampl\n",
    "    \n",
    "    def src_tokenize(self, ampl, seed):\n",
    "        \"\"\"Tokenize source expression, optionally applying replacements.\"\"\"\n",
    "        temp_ampl = self.src_replace(ampl, seed) if self.to_replace else ampl\n",
    "        temp_ampl = temp_ampl.replace('\\\\\\\\', '\\\\').replace('\\\\', ' \\\\ ').replace('%', '')\n",
    "        \n",
    "        for symbol, pattern in self.pattern_operators.items():\n",
    "            temp_ampl = pattern.sub(f' {symbol} ', temp_ampl)\n",
    "\n",
    "        temp_ampl = self.function_opening.sub(r'\\1 ', temp_ampl)\n",
    "        \n",
    "        temp_ampl = re.sub(r' {2,}', ' ', temp_ampl)\n",
    "        return [token for token in self.split_expression(temp_ampl) if token]\n",
    "\n",
    "    def tgt_tokenize(self, sqampl):\n",
    "        \"\"\"Tokenize target expression.\"\"\"\n",
    "        sqampl = self.remove_whitespace(sqampl)\n",
    "        temp_sqampl = sqampl\n",
    "        \n",
    "        for symbol, pattern in self.pattern_operators.items():\n",
    "            temp_sqampl = pattern.sub(f' {symbol} ', temp_sqampl)\n",
    "        \n",
    "        for pattern in [self.pattern_reg_prop, self.pattern_mass, self.pattern_s]:\n",
    "            temp_sqampl = pattern.sub(lambda match: f' {match.group(0)} ', temp_sqampl)\n",
    "        \n",
    "        temp_sqampl = re.sub(r' {2,}', ' ', temp_sqampl)\n",
    "        return [token for token in self.split_expression(temp_sqampl) if token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27373bed",
   "metadata": {
    "papermill": {
     "duration": 0.006073,
     "end_time": "2025-04-02T18:19:34.422653",
     "exception": false,
     "start_time": "2025-04-02T18:19:34.416580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94066ac7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T18:19:34.436058Z",
     "iopub.status.busy": "2025-04-02T18:19:34.435845Z",
     "iopub.status.idle": "2025-04-02T18:19:34.445501Z",
     "shell.execute_reply": "2025-04-02T18:19:34.444898Z"
    },
    "papermill": {
     "duration": 0.017678,
     "end_time": "2025-04-02T18:19:34.446735",
     "exception": false,
     "start_time": "2025-04-02T18:19:34.429057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BOS_IDX, PAD_IDX, EOS_IDX, UNK_IDX, SEP_IDX = 0, 1, 2, 3, 4\n",
    "special_symbols = ['<S>', '<PAD>', '</S>', '<UNK>', '<SEP>']\n",
    "tokenizer = Tokenizer(train_df, 500, 500, special_symbols, UNK_IDX, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2170d715",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T18:19:34.460171Z",
     "iopub.status.busy": "2025-04-02T18:19:34.459966Z",
     "iopub.status.idle": "2025-04-02T18:19:34.468691Z",
     "shell.execute_reply": "2025-04-02T18:19:34.468076Z"
    },
    "papermill": {
     "duration": 0.017029,
     "end_time": "2025-04-02T18:19:34.470064",
     "exception": false,
     "start_time": "2025-04-02T18:19:34.453035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_indices(tokenizer, expressions, index_token_pool_size=50, momentum_token_pool_size=50):\n",
    "    # Function to replace indices with a new set of tokens for each expression\n",
    "    def replace_indices(token_list, index_map):\n",
    "        new_index = (f\"_{i}\" for i in range(index_token_pool_size))  # Local generator for new indices\n",
    "        new_tokens = []\n",
    "        for token in token_list:\n",
    "            if \"INDEX_\" in token:\n",
    "                if token not in index_map:\n",
    "                    try:\n",
    "                        index_map[token] = token.rsplit('_',1)[0] + next(new_index)\n",
    "                    except StopIteration:\n",
    "                        # Handle the case where no more indices are available\n",
    "                        raise ValueError(\"Ran out of unique indices, increase token_pool_size\")\n",
    "                new_tokens.append(index_map[token])\n",
    "            else:\n",
    "                new_tokens.append(token)\n",
    "        return new_tokens\n",
    "\n",
    "    def replace_momenta(token_list, index_map):\n",
    "        new_index = (f\"MOMENTUM_{i}\" for i in range(momentum_token_pool_size))  # Local generator for new indices\n",
    "        new_tokens = []\n",
    "        for token in token_list:\n",
    "            if \"MOMENTUM_\" in token:\n",
    "                if token not in index_map:\n",
    "                    try:\n",
    "                        index_map[token] = next(new_index)\n",
    "                    except StopIteration:\n",
    "                        # Handle the case where no more indices are available\n",
    "                        raise ValueError(\"Ran out of unique indices, increase momentum_token_pool_size\")\n",
    "                new_tokens.append(index_map[token])\n",
    "            else:\n",
    "                new_tokens.append(token)\n",
    "        return new_tokens\n",
    "\n",
    "    normalized_expressions = []\n",
    "    # Replace indices in each expression randomly\n",
    "    for expr in tqdm(expressions,desc=\"Normalizing..\"):\n",
    "        toks = tokenizer.src_tokenize(expr,42)\n",
    "        normalized_expressions.append(replace_momenta(replace_indices(toks, {}), {}))\n",
    "\n",
    "    return normalized_expressions\n",
    "\n",
    "\n",
    "def aug_data(df):\n",
    "    # Extract columns\n",
    "    amps = df['Amplitude']\n",
    "    sqamps = df['Squared Amplitude']\n",
    "\n",
    "    # Data augmentation\n",
    "    n_samples = 1 #args.n_samples\n",
    "    aug_amps = []\n",
    "\n",
    "    for amp in tqdm(amps, desc='processing'):\n",
    "        random_seed = [random.randint(1, 1000) for _ in range(n_samples)]\n",
    "        for seed in random_seed:\n",
    "            aug_amps.append(tokenizer.src_replace(amp, seed))\n",
    "    aug_sqamps = [sqamp for sqamp in sqamps for _ in range(n_samples)]\n",
    "\n",
    "    if True:\n",
    "        normal_amps = normalize_indices(tokenizer, aug_amps, 500, 500)\n",
    "        aug_amps = []\n",
    "        for amp in normal_amps:\n",
    "            aug_amps.append(\"\".join(amp))\n",
    "\n",
    "    # Create augmented DataFrame\n",
    "    df_aug = pd.DataFrame({\"Amplitude\": aug_amps, \"Squared Amplitude\": aug_sqamps})\n",
    "\n",
    "    return df_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0984cf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T18:19:34.483888Z",
     "iopub.status.busy": "2025-04-02T18:19:34.483655Z",
     "iopub.status.idle": "2025-04-02T18:19:43.069715Z",
     "shell.execute_reply": "2025-04-02T18:19:43.068808Z"
    },
    "papermill": {
     "duration": 8.59428,
     "end_time": "2025-04-02T18:19:43.071262",
     "exception": false,
     "start_time": "2025-04-02T18:19:34.476982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing: 100%|██████████| 12441/12441 [00:06<00:00, 1945.11it/s]\n",
      "Normalizing..: 100%|██████████| 12441/12441 [00:02<00:00, 5848.00it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df_aug = aug_data(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8055297f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T18:19:43.093965Z",
     "iopub.status.busy": "2025-04-02T18:19:43.093699Z",
     "iopub.status.idle": "2025-04-02T18:19:45.235206Z",
     "shell.execute_reply": "2025-04-02T18:19:45.234190Z"
    },
    "papermill": {
     "duration": 2.153852,
     "end_time": "2025-04-02T18:19:45.236410",
     "exception": false,
     "start_time": "2025-04-02T18:19:43.082558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing: 100%|██████████| 1555/1555 [00:00<00:00, 1973.02it/s]\n",
      "Normalizing..: 100%|██████████| 1555/1555 [00:00<00:00, 5970.36it/s]\n",
      "processing: 100%|██████████| 1556/1556 [00:00<00:00, 1959.23it/s]\n",
      "Normalizing..: 100%|██████████| 1556/1556 [00:00<00:00, 5831.01it/s]\n"
     ]
    }
   ],
   "source": [
    "val_df_aug = aug_data(val_df)\n",
    "test_df_aug = aug_data(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "340f61c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T18:19:45.260866Z",
     "iopub.status.busy": "2025-04-02T18:19:45.260576Z",
     "iopub.status.idle": "2025-04-02T18:19:45.492797Z",
     "shell.execute_reply": "2025-04-02T18:19:45.491837Z"
    },
    "papermill": {
     "duration": 0.245902,
     "end_time": "2025-04-02T18:19:45.494361",
     "exception": false,
     "start_time": "2025-04-02T18:19:45.248459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df_aug.to_csv('train_df_aug.csv')\n",
    "val_df_aug.to_csv('val_df_aug.csv')\n",
    "test_df_aug.to_csv('test_df_aug.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcdfe083",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T18:19:45.518740Z",
     "iopub.status.busy": "2025-04-02T18:19:45.518461Z",
     "iopub.status.idle": "2025-04-02T18:19:45.523477Z",
     "shell.execute_reply": "2025-04-02T18:19:45.522661Z"
    },
    "papermill": {
     "duration": 0.018557,
     "end_time": "2025-04-02T18:19:45.524795",
     "exception": false,
     "start_time": "2025-04-02T18:19:45.506238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer2 = Tokenizer(train_df_aug, 500, 500, special_symbols, UNK_IDX, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c832b043",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T18:19:45.548649Z",
     "iopub.status.busy": "2025-04-02T18:19:45.548410Z",
     "iopub.status.idle": "2025-04-02T18:19:47.145239Z",
     "shell.execute_reply": "2025-04-02T18:19:47.144321Z"
    },
    "papermill": {
     "duration": 1.610072,
     "end_time": "2025-04-02T18:19:47.146482",
     "exception": false,
     "start_time": "2025-04-02T18:19:45.536410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing source vocab: 100%|██████████| 12441/12441 [00:01<00:00, 7832.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "459"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab2 = tokenizer2.build_src_vocab(42)\n",
    "len(src_vocab2.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fae7696",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T18:19:47.172679Z",
     "iopub.status.busy": "2025-04-02T18:19:47.172374Z",
     "iopub.status.idle": "2025-04-02T18:19:48.721547Z",
     "shell.execute_reply": "2025-04-02T18:19:48.720538Z"
    },
    "papermill": {
     "duration": 1.563734,
     "end_time": "2025-04-02T18:19:48.722825",
     "exception": false,
     "start_time": "2025-04-02T18:19:47.159091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing target vocab: 100%|██████████| 12441/12441 [00:01<00:00, 8063.27it/s]\n"
     ]
    }
   ],
   "source": [
    "tgt_vocab2 = tokenizer2.build_tgt_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e11cf32d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T18:19:48.750880Z",
     "iopub.status.busy": "2025-04-02T18:19:48.750604Z",
     "iopub.status.idle": "2025-04-02T18:19:48.755017Z",
     "shell.execute_reply": "2025-04-02T18:19:48.754324Z"
    },
    "papermill": {
     "duration": 0.019875,
     "end_time": "2025-04-02T18:19:48.756314",
     "exception": false,
     "start_time": "2025-04-02T18:19:48.736439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tgt_vocab2.itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc38acd",
   "metadata": {
    "papermill": {
     "duration": 0.013054,
     "end_time": "2025-04-02T18:19:48.782706",
     "exception": false,
     "start_time": "2025-04-02T18:19:48.769652",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Torch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4cc44ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T18:19:48.810342Z",
     "iopub.status.busy": "2025-04-02T18:19:48.810016Z",
     "iopub.status.idle": "2025-04-02T18:19:48.821651Z",
     "shell.execute_reply": "2025-04-02T18:19:48.820989Z"
    },
    "papermill": {
     "duration": 0.026898,
     "end_time": "2025-04-02T18:19:48.822838",
     "exception": false,
     "start_time": "2025-04-02T18:19:48.795940",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "def causal_mask(size):\n",
    "    \"\"\"Create a causal mask for a sequence of given size.\"\"\"\n",
    "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).int()\n",
    "    return mask == 0\n",
    "    \n",
    "class Data(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch dataset for handling data.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame containing data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, tokenizer, config, src_vocab, tgt_vocab):\n",
    "        super(Data, self).__init__()\n",
    "        self.tgt_vals = df['Squared Amplitude']\n",
    "        self.src_vals = df['Amplitude']\n",
    "        self.tgt_tokenize = tokenizer.tgt_tokenize\n",
    "        self.src_tokenize = tokenizer.src_tokenize\n",
    "        self.bos_token = torch.tensor([BOS_IDX], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([EOS_IDX], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([PAD_IDX], dtype=torch.int64)\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.config = config\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get the length of the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Length of the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.src_vals)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get an item from the dataset at the specified index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the item.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Tuple containing source and target tensors.\n",
    "        \"\"\"\n",
    "        # print(f'index: {idx}')\n",
    "        src_tokenized = self.src_tokenize(self.src_vals[idx],self.config.seed)\n",
    "        tgt_tokenized = self.tgt_tokenize(self.tgt_vals[idx])\n",
    "        src_ids = self.src_vocab.forward(src_tokenized)\n",
    "        tgt_ids = self.tgt_vocab.forward(tgt_tokenized)\n",
    "\n",
    "        enc_num_padding_tokens = self.config.src_max_len - len(src_ids) - 2\n",
    "        dec_num_padding_tokens = self.config.tgt_max_len - len(tgt_ids) - 1\n",
    "        # print(f'src_ids: {len(src_ids)} tgt_ids:  {len(tgt_ids)} enc_num: {enc_num_padding_tokens} dec_num: {dec_num_padding_tokens} \\n' )\n",
    "        if self.config.truncate:\n",
    "            if enc_num_padding_tokens < 0:\n",
    "                src_ids = src_ids[:self.config.src_max_len-2]\n",
    "            if dec_num_padding_tokens < 0:\n",
    "                tgt_ids = tgt_ids[:self.config.tgt_max_len-1]\n",
    "        else:\n",
    "            if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "                raise ValueError(\"Sentence is too long\")\n",
    "        src_tensor = torch.cat(\n",
    "            [\n",
    "                self.bos_token,\n",
    "                torch.tensor(src_ids, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] *\n",
    "                             enc_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        tgt_tensor = torch.cat(\n",
    "            [\n",
    "                self.bos_token,\n",
    "                torch.tensor(tgt_ids, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] *\n",
    "                             dec_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(tgt_ids, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        src_mask = (src_tensor != self.pad_token).unsqueeze(0).unsqueeze(0).int() # (1, 1, seq_len)\n",
    "        tgt_mask = (tgt_tensor != self.pad_token).unsqueeze(0).int() & causal_mask(tgt_tensor.size(0)) # (1, seq_len) & (1, seq_len, seq_len),\n",
    "\n",
    "        return src_tensor, tgt_tensor, label, src_mask, tgt_mask#, len(src_ids), len(tgt_ids)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_data(df_train, df_test, df_valid, config, tokenizer, src_vocab,tgt_vocab):\n",
    "        \"\"\"\n",
    "        Create datasets (train, test, and valid)\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing train, test, and valid datasets.\n",
    "        \"\"\"\n",
    "        train = Data(df_train, tokenizer, config,src_vocab,tgt_vocab)\n",
    "        test = Data(df_test, tokenizer, config,src_vocab,tgt_vocab)\n",
    "        valid = Data(df_valid, tokenizer, config,src_vocab,tgt_vocab)\n",
    "\n",
    "        return {'train': train, 'test': test, 'valid': valid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ab03afe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T18:19:48.850183Z",
     "iopub.status.busy": "2025-04-02T18:19:48.849937Z",
     "iopub.status.idle": "2025-04-02T18:19:48.853060Z",
     "shell.execute_reply": "2025-04-02T18:19:48.852468Z"
    },
    "papermill": {
     "duration": 0.018302,
     "end_time": "2025-04-02T18:19:48.854350",
     "exception": false,
     "start_time": "2025-04-02T18:19:48.836048",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class data_config:\n",
    "    src_max_len = 300\n",
    "    tgt_max_len = 325\n",
    "    truncate = False\n",
    "    seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f610b03a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T18:19:48.881743Z",
     "iopub.status.busy": "2025-04-02T18:19:48.881450Z",
     "iopub.status.idle": "2025-04-02T18:19:48.899194Z",
     "shell.execute_reply": "2025-04-02T18:19:48.898254Z"
    },
    "papermill": {
     "duration": 0.033016,
     "end_time": "2025-04-02T18:19:48.900667",
     "exception": false,
     "start_time": "2025-04-02T18:19:48.867651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = Data.get_data(train_df_aug, test_df_aug, val_df_aug, data_config, tokenizer2, src_vocab2, tgt_vocab2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e16292e",
   "metadata": {
    "papermill": {
     "duration": 0.013154,
     "end_time": "2025-04-02T18:19:48.927348",
     "exception": false,
     "start_time": "2025-04-02T18:19:48.914194",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acdf73d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T18:19:48.956514Z",
     "iopub.status.busy": "2025-04-02T18:19:48.956178Z",
     "iopub.status.idle": "2025-04-02T18:19:52.689595Z",
     "shell.execute_reply": "2025-04-02T18:19:52.688885Z"
    },
    "papermill": {
     "duration": 3.749215,
     "end_time": "2025-04-02T18:19:52.691212",
     "exception": false,
     "start_time": "2025-04-02T18:19:48.941997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from x_transformers import XTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63ef1021",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T18:19:52.719961Z",
     "iopub.status.busy": "2025-04-02T18:19:52.719494Z",
     "iopub.status.idle": "2025-04-02T18:19:52.777337Z",
     "shell.execute_reply": "2025-04-02T18:19:52.776604Z"
    },
    "papermill": {
     "duration": 0.073204,
     "end_time": "2025-04-02T18:19:52.778642",
     "exception": false,
     "start_time": "2025-04-02T18:19:52.705438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 16\n",
    "# learning_rate = 5e-5\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create dataloaders\n",
    "dataloaders = {\n",
    "    split: DataLoader(\n",
    "        dataset[split],\n",
    "        batch_size= batch_size if split == 'train' else 64,\n",
    "        shuffle=(split == 'train'),\n",
    "        pin_memory=True,\n",
    "        num_workers=4\n",
    "    ) for split in ['train', 'valid', 'test']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0294c749",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T18:19:52.806274Z",
     "iopub.status.busy": "2025-04-02T18:19:52.805997Z",
     "iopub.status.idle": "2025-04-02T18:19:52.812296Z",
     "shell.execute_reply": "2025-04-02T18:19:52.811660Z"
    },
    "papermill": {
     "duration": 0.021357,
     "end_time": "2025-04-02T18:19:52.813589",
     "exception": false,
     "start_time": "2025-04-02T18:19:52.792232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, dataloader, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    total_seq = 0\n",
    "    correct_seq = 0\n",
    "    total_tokens = 0\n",
    "    correct_tokens = 0\n",
    "    loss_sum = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Calculating accuracy\"):\n",
    "            # Move batch to device\n",
    "            src_tensor, tgt_tensor, label, src_mask, tgt_mask = batch\n",
    "            start_tokens = (torch.zeros(src_tensor.shape[0],1)).long().cuda()\n",
    "            src_tensor = src_tensor.to(device)\n",
    "            tgt_tensor = tgt_tensor.to(device)\n",
    "            label = label.to(device)\n",
    "            src_mask = src_mask.squeeze(1,2).to(device=device,dtype=torch.bool)\n",
    "            tgt_mask = tgt_mask.squeeze(1,2).to(device=device,dtype=torch.bool)\n",
    "            # Generate predictions\n",
    "            outputs = model.generate(\n",
    "                src_tensor, start_tokens, 325, mask = src_mask)\n",
    "            \n",
    "            total_tokens += outputs.numel()\n",
    "            correct_tokens += ((outputs == label)).sum().item()\n",
    "            \n",
    "            batch_size = label.size(0)\n",
    "            seq_correct = torch.all((outputs == label), dim=1)\n",
    "            correct_seq += seq_correct.sum().item()\n",
    "            total_seq += batch_size\n",
    "    \n",
    "    # Calculate metrics\n",
    "    token_accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0\n",
    "    sequence_accuracy = correct_seq / total_seq if total_seq > 0 else 0\n",
    "    avg_loss = loss_sum / len(dataloader)\n",
    "    \n",
    "    return {\n",
    "        'token_accuracy': token_accuracy,\n",
    "        'sequence_accuracy': sequence_accuracy,\n",
    "        'loss': avg_loss\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "576421a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T18:19:52.841269Z",
     "iopub.status.busy": "2025-04-02T18:19:52.841003Z",
     "iopub.status.idle": "2025-04-02T18:19:52.844929Z",
     "shell.execute_reply": "2025-04-02T18:19:52.844141Z"
    },
    "papermill": {
     "duration": 0.018874,
     "end_time": "2025-04-02T18:19:52.846089",
     "exception": false,
     "start_time": "2025-04-02T18:19:52.827215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_line_params(point1, point2):\n",
    "\n",
    "    x1, y1 = point1\n",
    "    x2, y2 = point2\n",
    "\n",
    "    # Check if the x coordinates are the same to avoid division by zero\n",
    "    if x1 == x2:\n",
    "        raise ValueError(\n",
    "            \"The x coordinates of the two points must be different to define a straight line.\")\n",
    "\n",
    "    # Calculate the slope (m)\n",
    "    m = (y2 - y1) / (x2 - x1)\n",
    "\n",
    "    # Calculate the intercept (b)\n",
    "    b = y1 - m * x1\n",
    "\n",
    "    return m, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "339559e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T18:19:52.873096Z",
     "iopub.status.busy": "2025-04-02T18:19:52.872848Z",
     "iopub.status.idle": "2025-04-02T18:19:52.876417Z",
     "shell.execute_reply": "2025-04-02T18:19:52.875637Z"
    },
    "papermill": {
     "duration": 0.018467,
     "end_time": "2025-04-02T18:19:52.877627",
     "exception": false,
     "start_time": "2025-04-02T18:19:52.859160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_decay = 0.01\n",
    "# num_epochs = 12\n",
    "grad_accumulation_steps = 1\n",
    "max_grad_norm = 1.0\n",
    "warmup_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d30be8c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T18:19:52.904741Z",
     "iopub.status.busy": "2025-04-02T18:19:52.904493Z",
     "iopub.status.idle": "2025-04-02T19:38:42.485379Z",
     "shell.execute_reply": "2025-04-02T19:38:42.484091Z"
    },
    "papermill": {
     "duration": 4729.596297,
     "end_time": "2025-04-02T19:38:42.486966",
     "exception": false,
     "start_time": "2025-04-02T18:19:52.890669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 [Train]:  64%|██████▍   | 501/778 [01:28<00:48,  5.75it/s, loss=0.389]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup completed!\n",
      "Warmup completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 [Train]: 100%|██████████| 778/778 [02:17<00:00,  5.68it/s, loss=0.258]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR changed to: [0.0003]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 [Valid]: 100%|██████████| 25/25 [00:06<00:00,  3.99it/s, loss=0.00814]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - Train Loss: 0.25751, Valid Loss: 0.00814\n",
      "Model checkpoint saved (Valid Loss: 0.00814)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 [Train]: 100%|██████████| 778/778 [02:15<00:00,  5.74it/s, loss=0.00576]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR changed to: [0.0002900003333333333]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 [Valid]: 100%|██████████| 25/25 [00:06<00:00,  4.15it/s, loss=0.00459]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 - Train Loss: 0.00576, Valid Loss: 0.00459\n",
      "Model checkpoint saved (Valid Loss: 0.00459)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 [Train]: 100%|██████████| 778/778 [02:15<00:00,  5.74it/s, loss=0.00403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR changed to: [0.0002800006666666666]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 [Valid]: 100%|██████████| 25/25 [00:05<00:00,  4.17it/s, loss=0.00329]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 - Train Loss: 0.00403, Valid Loss: 0.00329\n",
      "Model checkpoint saved (Valid Loss: 0.00329)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 [Train]: 100%|██████████| 778/778 [02:15<00:00,  5.75it/s, loss=0.00258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR changed to: [0.00027000099999999996]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 [Valid]: 100%|██████████| 25/25 [00:05<00:00,  4.17it/s, loss=0.00591]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 - Train Loss: 0.00258, Valid Loss: 0.00591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 [Train]: 100%|██████████| 778/778 [02:15<00:00,  5.76it/s, loss=0.00323]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR changed to: [0.0002600013333333333]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 [Valid]: 100%|██████████| 25/25 [00:06<00:00,  4.15it/s, loss=0.00179]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 - Train Loss: 0.00323, Valid Loss: 0.00179\n",
      "Model checkpoint saved (Valid Loss: 0.00179)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-3022da5f9609>:148: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  tmodel.load_state_dict(torch.load(f'/kaggle/working/best_model_checkpoint_{best_valid_epoch}.pt')['model_state_dict'])\n",
      "Calculating accuracy: 100%|██████████| 25/25 [01:19<00:00,  3.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_accuracy': 0.9208166897369983, 'sequence_accuracy': 0.788560411311054, 'loss': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 [Train]: 100%|██████████| 778/778 [02:15<00:00,  5.74it/s, loss=0.00232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR changed to: [0.00025000166666666666]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 [Valid]: 100%|██████████| 25/25 [00:06<00:00,  4.13it/s, loss=0.0131]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 - Train Loss: 0.00232, Valid Loss: 0.01308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 [Train]: 100%|██████████| 778/778 [02:15<00:00,  5.75it/s, loss=0.00225]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR changed to: [0.00024000199999999993]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 [Valid]: 100%|██████████| 25/25 [00:06<00:00,  4.16it/s, loss=0.00532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 - Train Loss: 0.00225, Valid Loss: 0.00532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 [Train]: 100%|██████████| 778/778 [02:15<00:00,  5.75it/s, loss=0.00188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR changed to: [0.0002300023333333333]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 [Valid]: 100%|██████████| 25/25 [00:06<00:00,  4.10it/s, loss=0.00224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 - Train Loss: 0.00188, Valid Loss: 0.00224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 [Train]: 100%|██████████| 778/778 [02:15<00:00,  5.74it/s, loss=0.00167]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR changed to: [0.00022000266666666665]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 [Valid]: 100%|██████████| 25/25 [00:06<00:00,  4.16it/s, loss=0.00245]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 - Train Loss: 0.00167, Valid Loss: 0.00245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 [Train]: 100%|██████████| 778/778 [02:15<00:00,  5.75it/s, loss=0.00136]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR changed to: [0.00021000299999999995]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 [Valid]: 100%|██████████| 25/25 [00:06<00:00,  4.10it/s, loss=0.00162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 - Train Loss: 0.00136, Valid Loss: 0.00162\n",
      "Model checkpoint saved (Valid Loss: 0.00162)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating accuracy: 100%|██████████| 25/25 [01:18<00:00,  3.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_accuracy': 0.9237314613407158, 'sequence_accuracy': 0.7795629820051414, 'loss': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 [Train]: 100%|██████████| 778/778 [02:15<00:00,  5.75it/s, loss=0.00153]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR changed to: [0.0002000033333333333]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 [Valid]: 100%|██████████| 25/25 [00:05<00:00,  4.17it/s, loss=0.00421]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 - Train Loss: 0.00153, Valid Loss: 0.00421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 [Train]: 100%|██████████| 778/778 [02:15<00:00,  5.75it/s, loss=0.00111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR changed to: [0.00019000366666666664]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 [Valid]: 100%|██████████| 25/25 [00:06<00:00,  4.13it/s, loss=0.00138]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 - Train Loss: 0.00111, Valid Loss: 0.00138\n",
      "Model checkpoint saved (Valid Loss: 0.00138)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 [Train]: 100%|██████████| 778/778 [02:15<00:00,  5.75it/s, loss=0.00109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR changed to: [0.00018000399999999997]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 [Valid]: 100%|██████████| 25/25 [00:06<00:00,  4.16it/s, loss=0.00278]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 - Train Loss: 0.00109, Valid Loss: 0.00278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 [Train]: 100%|██████████| 778/778 [02:15<00:00,  5.75it/s, loss=0.000747]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR changed to: [0.00017000433333333331]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 [Valid]: 100%|██████████| 25/25 [00:06<00:00,  4.10it/s, loss=0.00104]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 - Train Loss: 0.00075, Valid Loss: 0.00104\n",
      "Model checkpoint saved (Valid Loss: 0.00104)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 [Train]: 100%|██████████| 778/778 [02:15<00:00,  5.75it/s, loss=0.000987]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR changed to: [0.0001600046666666666]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 [Valid]: 100%|██████████| 25/25 [00:06<00:00,  4.15it/s, loss=0.000516]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 - Train Loss: 0.00099, Valid Loss: 0.00052\n",
      "Model checkpoint saved (Valid Loss: 0.00052)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating accuracy: 100%|██████████| 25/25 [01:18<00:00,  3.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_accuracy': 0.9527921692703184, 'sequence_accuracy': 0.8933161953727506, 'loss': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 [Train]: 100%|██████████| 778/778 [02:15<00:00,  5.75it/s, loss=0.000905]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR changed to: [0.00015000499999999998]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 [Valid]: 100%|██████████| 25/25 [00:06<00:00,  4.16it/s, loss=0.00131]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 - Train Loss: 0.00091, Valid Loss: 0.00131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 [Train]: 100%|██████████| 778/778 [02:15<00:00,  5.75it/s, loss=0.000845]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR changed to: [0.00014000533333333328]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 [Valid]: 100%|██████████| 25/25 [00:06<00:00,  4.16it/s, loss=0.000525]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 - Train Loss: 0.00084, Valid Loss: 0.00052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 [Train]: 100%|██████████| 778/778 [02:15<00:00,  5.75it/s, loss=0.000462]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR changed to: [0.00013000566666666663]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 [Valid]: 100%|██████████| 25/25 [00:06<00:00,  4.13it/s, loss=0.000494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 - Train Loss: 0.00046, Valid Loss: 0.00049\n",
      "Model checkpoint saved (Valid Loss: 0.00049)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 [Train]: 100%|██████████| 778/778 [02:15<00:00,  5.75it/s, loss=0.000422]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR changed to: [0.00012000599999999998]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 [Valid]: 100%|██████████| 25/25 [00:06<00:00,  4.14it/s, loss=0.000536]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 - Train Loss: 0.00042, Valid Loss: 0.00054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 [Train]: 100%|██████████| 778/778 [02:15<00:00,  5.75it/s, loss=0.000899]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR changed to: [0.0001100063333333333]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 [Valid]: 100%|██████████| 25/25 [00:06<00:00,  4.15it/s, loss=0.000547]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 - Train Loss: 0.00090, Valid Loss: 0.00055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating accuracy: 100%|██████████| 25/25 [01:18<00:00,  3.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_accuracy': 0.9539450266956694, 'sequence_accuracy': 0.9010282776349614, 'loss': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 [Train]: 100%|██████████| 778/778 [02:15<00:00,  5.75it/s, loss=0.000407]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR changed to: [0.00010000666666666665]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 [Valid]: 100%|██████████| 25/25 [00:06<00:00,  4.10it/s, loss=0.000466]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 - Train Loss: 0.00041, Valid Loss: 0.00047\n",
      "Model checkpoint saved (Valid Loss: 0.00047)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 [Train]: 100%|██████████| 778/778 [02:15<00:00,  5.75it/s, loss=0.000409]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR changed to: [9.000699999999997e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 [Valid]: 100%|██████████| 25/25 [00:06<00:00,  4.16it/s, loss=0.000524]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 - Train Loss: 0.00041, Valid Loss: 0.00052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 [Train]: 100%|██████████| 778/778 [02:15<00:00,  5.75it/s, loss=0.000547]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR changed to: [8.00073333333333e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 [Valid]: 100%|██████████| 25/25 [00:06<00:00,  4.14it/s, loss=0.000417]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 - Train Loss: 0.00055, Valid Loss: 0.00042\n",
      "Model checkpoint saved (Valid Loss: 0.00042)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 [Train]: 100%|██████████| 778/778 [02:15<00:00,  5.75it/s, loss=0.000297]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR changed to: [7.000766666666664e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 [Valid]: 100%|██████████| 25/25 [00:06<00:00,  4.15it/s, loss=0.000381]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 - Train Loss: 0.00030, Valid Loss: 0.00038\n",
      "Model checkpoint saved (Valid Loss: 0.00038)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 [Train]: 100%|██████████| 778/778 [02:15<00:00,  5.75it/s, loss=0.000395]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR changed to: [6.000799999999996e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 [Valid]: 100%|██████████| 25/25 [00:06<00:00,  4.14it/s, loss=0.000556]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 - Train Loss: 0.00040, Valid Loss: 0.00056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating accuracy: 100%|██████████| 25/25 [01:18<00:00,  3.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_accuracy': 0.9684496737195966, 'sequence_accuracy': 0.9408740359897172, 'loss': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 [Train]: 100%|██████████| 778/778 [02:15<00:00,  5.75it/s, loss=0.000307]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR changed to: [5.0008333333333307e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 [Valid]: 100%|██████████| 25/25 [00:05<00:00,  4.17it/s, loss=0.000376]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 - Train Loss: 0.00031, Valid Loss: 0.00038\n",
      "Model checkpoint saved (Valid Loss: 0.00038)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 [Train]: 100%|██████████| 778/778 [02:15<00:00,  5.74it/s, loss=0.000284]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR changed to: [4.0008666666666655e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 [Valid]: 100%|██████████| 25/25 [00:06<00:00,  4.13it/s, loss=0.000376]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 - Train Loss: 0.00028, Valid Loss: 0.00038\n",
      "Model checkpoint saved (Valid Loss: 0.00038)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 [Train]: 100%|██████████| 778/778 [02:15<00:00,  5.74it/s, loss=0.000269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR changed to: [3.000899999999995e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 [Valid]: 100%|██████████| 25/25 [00:05<00:00,  4.17it/s, loss=0.000389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 - Train Loss: 0.00027, Valid Loss: 0.00039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 [Train]: 100%|██████████| 778/778 [02:15<00:00,  5.74it/s, loss=0.000253]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR changed to: [2.0009333333333298e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 [Valid]: 100%|██████████| 25/25 [00:06<00:00,  4.15it/s, loss=0.000378]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 - Train Loss: 0.00025, Valid Loss: 0.00038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 [Train]: 100%|██████████| 778/778 [02:15<00:00,  5.73it/s, loss=0.000243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR changed to: [1.0009666666666646e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 [Valid]: 100%|██████████| 25/25 [00:06<00:00,  4.14it/s, loss=0.00039]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 - Train Loss: 0.00024, Valid Loss: 0.00039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating accuracy: 100%|██████████| 25/25 [01:18<00:00,  3.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_accuracy': 0.9568182717025905, 'sequence_accuracy': 0.9158097686375322, 'loss': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 25/25 [00:06<00:00,  4.13it/s, loss=0.000481]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0005\n",
      "Training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import copy\n",
    "\n",
    "model = XTransformer(\n",
    "    dim = 512,\n",
    "    enc_num_tokens = 459,\n",
    "    enc_depth = 2,\n",
    "    enc_heads = 8,\n",
    "    enc_max_seq_len = 300,\n",
    "    dec_num_tokens = 59,\n",
    "    dec_depth = 4,\n",
    "    dec_heads = 8,\n",
    "    dec_max_seq_len = 325,\n",
    "    # mult=8,\n",
    "    tie_token_emb = False,      # tie embeddings of encoder and decoder\n",
    ")\n",
    "\n",
    "\n",
    "start_lr = 3e-4\n",
    "end_lr = 1e-8\n",
    "warmup_steps = 500\n",
    "num_epochs = 30\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=start_lr, weight_decay=weight_decay)\n",
    "m_warm, c_warm = calculate_line_params( (0, end_lr), (warmup_steps, start_lr))\n",
    "\n",
    "def lam_warm(step): \n",
    "    return (1/start_lr)*(m_warm*step + c_warm)\n",
    "                        \n",
    "warm_scheduler = LambdaLR(optimizer, lr_lambda=lam_warm)\n",
    "\n",
    "m_decay, c_decay = calculate_line_params( (0, start_lr), (num_epochs, end_lr))\n",
    "\n",
    "def lam(epoch): \n",
    "    return (1/start_lr) * (m_decay*epoch + c_decay)\n",
    "    \n",
    "lr_scheduler = LambdaLR(optimizer, lr_lambda=lam)\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "global_steps = 0\n",
    "\n",
    "# Training loop\n",
    "best_valid_loss = float('inf')\n",
    "best_valid_epoch = 0\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_steps = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloaders['train'], desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "    for i, batch in enumerate(progress_bar):\n",
    "        \n",
    "        # Move batch to device\n",
    "        src_tensor, tgt_tensor, label, src_mask, tgt_mask = batch\n",
    "        \n",
    "        src_tensor = src_tensor.to(device)\n",
    "        tgt_tensor = tgt_tensor.to(device)\n",
    "        src_mask = src_mask.squeeze(1,2).to(device=device,dtype=torch.bool)\n",
    "        \n",
    "        # Forward pass\n",
    "        loss = model(src_tensor, tgt_tensor, mask = src_mask)\n",
    "        loss = loss / grad_accumulation_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update metrics\n",
    "        train_loss += loss.item() * grad_accumulation_steps\n",
    "        train_steps += 1\n",
    "        global_steps += 1\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({'loss': train_loss / train_steps})\n",
    "        \n",
    "        # Gradient accumulation\n",
    "        if (i + 1) % grad_accumulation_steps == 0:\n",
    "            # Clip gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            # scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            if global_steps < warmup_steps:\n",
    "                warm_scheduler.step()\n",
    "            elif global_steps <= warmup_steps + grad_accumulation_steps:\n",
    "                print('Warmup completed!')\n",
    "            \n",
    "        src_tensor = src_tensor.to('cpu')\n",
    "        tgt_tensor = tgt_tensor.to('cpu')\n",
    "        src_mask = src_mask.to(device='cpu')   \n",
    "\n",
    "    # Calculate average training loss\n",
    "    avg_train_loss = train_loss / train_steps\n",
    "    if global_steps > warmup_steps:\n",
    "        lr_scheduler.step(epoch)\n",
    "        print('LR changed to:', lr_scheduler.get_last_lr())\n",
    "        \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    valid_steps = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloaders['valid'], desc=f\"Epoch {epoch+1}/{num_epochs} [Valid]\")\n",
    "        for batch in progress_bar:\n",
    "            # Move batch to device\n",
    "            src_tensor, tgt_tensor, label, src_mask, tgt_mask = batch\n",
    "            \n",
    "            src_tensor = src_tensor.to(device)\n",
    "            tgt_tensor = tgt_tensor.to(device)\n",
    "            src_mask = src_mask.squeeze(1,2).to(device=device,dtype=torch.bool)\n",
    "            # Forward pass\n",
    "            loss = model(src_tensor, tgt_tensor, mask = src_mask)\n",
    "            \n",
    "            # Update metrics\n",
    "            valid_loss += loss.item()\n",
    "            valid_steps += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({'loss': valid_loss / valid_steps})\n",
    "            src_tensor = src_tensor.to('cpu')\n",
    "            tgt_tensor = tgt_tensor.to('cpu')\n",
    "            src_mask = src_mask.to(device='cpu')   \n",
    "            \n",
    "    # Calculate average validation loss\n",
    "    avg_valid_loss = valid_loss / valid_steps\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.5f}, Valid Loss: {avg_valid_loss:.5f}\")\n",
    "    \n",
    "    # Save checkpoint if validation loss improved\n",
    "    if avg_valid_loss < best_valid_loss:\n",
    "        best_valid_loss = avg_valid_loss\n",
    "        best_valid_epoch = epoch + 1\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "            'train_loss': avg_train_loss,\n",
    "            'valid_loss': avg_valid_loss,\n",
    "        }, f'best_model_checkpoint_{epoch+1}.pt')\n",
    "        print(f\"Model checkpoint saved (Valid Loss: {avg_valid_loss:.5f})\")\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        tmodel = copy.deepcopy(model)\n",
    "        tmodel.load_state_dict(torch.load(f'/kaggle/working/best_model_checkpoint_{best_valid_epoch}.pt')['model_state_dict'])\n",
    "        test_metrics = calculate_accuracy(tmodel, dataloaders['test'], device)\n",
    "        print(test_metrics)\n",
    "# Final evaluation on test set\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "test_steps = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    progress_bar = tqdm(dataloaders['test'], desc=\"Testing\")\n",
    "    for batch in progress_bar:\n",
    "        # Move batch to device\n",
    "        src_tensor, tgt_tensor, label, src_mask, tgt_mask = batch\n",
    "        # labels = batch['labels'].to(device) if 'labels' in batch else None\n",
    "        src_tensor = src_tensor.to(device)\n",
    "        tgt_tensor = tgt_tensor.to(device)\n",
    "        src_mask = src_mask.squeeze(1,2).to(device=device,dtype=torch.bool)\n",
    "        # Forward pass\n",
    "        loss = model(src_tensor, tgt_tensor, mask = src_mask)\n",
    "        \n",
    "        # Update metrics\n",
    "        test_loss += loss.item()\n",
    "        test_steps += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({'loss': test_loss / test_steps})\n",
    "        \n",
    "        src_tensor = src_tensor.to('cpu')\n",
    "        tgt_tensor = tgt_tensor.to('cpu')\n",
    "        src_mask = src_mask.to(device='cpu')   \n",
    "# Calculate average test loss\n",
    "avg_test_loss = test_loss / test_steps\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7f227",
   "metadata": {
    "papermill": {
     "duration": 2.430052,
     "end_time": "2025-04-02T19:38:47.293692",
     "exception": false,
     "start_time": "2025-04-02T19:38:44.863640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4dbd5f15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T19:38:52.033662Z",
     "iopub.status.busy": "2025-04-02T19:38:52.033318Z",
     "iopub.status.idle": "2025-04-02T19:38:52.036837Z",
     "shell.execute_reply": "2025-04-02T19:38:52.035977Z"
    },
    "papermill": {
     "duration": 2.379473,
     "end_time": "2025-04-02T19:38:52.038111",
     "exception": false,
     "start_time": "2025-04-02T19:38:49.658638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # ckpt_path = '/kaggle/working/best_model_checkpoint_10.pt'\n",
    "# ckpt_path = '/kaggle/input/e1d1-trf-ep15/best_model_checkpoint_13.pt'\n",
    "# model.load_state_dict(torch.load(ckpt_path)['model_state_dict'])\n",
    "# calculate_accuracy(model, dataloaders['test'], device)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6729715,
     "sourceId": 10837100,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6765539,
     "sourceId": 10960275,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6842954,
     "sourceId": 10993577,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6846202,
     "sourceId": 10997893,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6931718,
     "sourceId": 11116869,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4775.780921,
   "end_time": "2025-04-02T19:38:56.256045",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-02T18:19:20.475124",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
