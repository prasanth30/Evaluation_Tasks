# Specific Task 3.2

## State Space Models

I have used Encoder-Decoder style Attention-SSM hybrid model, which has architecture similar to seq2seq transformer, replacing self-attention with Mamba kernels and keeping the cross attention intact.

## Implementation details

