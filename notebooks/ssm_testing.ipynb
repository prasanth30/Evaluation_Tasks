{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgXaTeg31Dc1",
        "trusted": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZgf016I1Dc1"
      },
      "source": [
        "# Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2025-04-01T09:53:56.059657Z",
          "iopub.status.busy": "2025-04-01T09:53:56.059287Z",
          "iopub.status.idle": "2025-04-01T09:53:57.155667Z",
          "shell.execute_reply": "2025-04-01T09:53:57.154831Z",
          "shell.execute_reply.started": "2025-04-01T09:53:56.05963Z"
        },
        "id": "xhqGKmoM1Dc2",
        "outputId": "37b00cf2-c595-45b2-94d0-0a2a8908e918",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'mamba'...\n",
            "remote: Enumerating objects: 723, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 723 (delta 26), reused 15 (delta 15), pack-reused 684 (from 3)\u001b[K\n",
            "Receiving objects: 100% (723/723), 1.58 MiB | 8.12 MiB/s, done.\n",
            "Resolving deltas: 100% (386/386), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/state-spaces/mamba.git\n",
        "# !git clone https://github.com/xtwigs/mamba.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T09:53:57.157352Z",
          "iopub.status.busy": "2025-04-01T09:53:57.157038Z",
          "iopub.status.idle": "2025-04-01T09:56:43.984702Z",
          "shell.execute_reply": "2025-04-01T09:56:43.983735Z",
          "shell.execute_reply.started": "2025-04-01T09:53:57.157326Z"
        },
        "id": "52BiXphK1Dc2",
        "outputId": "d5e9ad8e-4cea-4c0f-e2be-f296e01edd90",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for mamba_ssm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "CPU times: user 2.93 s, sys: 616 ms, total: 3.54 s\n",
            "Wall time: 2min 46s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# !pip install -q --cache-dir /kaggle/working/pip_cache/mamba_pkg /kaggle/working/mamba\n",
        "!pip install -q /kaggle/working/mamba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T09:56:43.986755Z",
          "iopub.status.busy": "2025-04-01T09:56:43.986455Z",
          "iopub.status.idle": "2025-04-01T09:56:58.672622Z",
          "shell.execute_reply": "2025-04-01T09:56:58.671457Z",
          "shell.execute_reply.started": "2025-04-01T09:56:43.986727Z"
        },
        "id": "xD5P3Zz21Dc2",
        "outputId": "86c9238f-665c-40db-d371-6872961d70b1",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 251 ms, sys: 72.8 ms, total: 323 ms\n",
            "Wall time: 14.7 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# !pip install -q --cache-dir /kaggle/working/pip_cache/causal_conv causal-conv1d>=1.4.0\n",
        "!pip install -q causal-conv1d>=1.4.0\n",
        "# !pip install -q mamba-ssm\n",
        "# !pip install -q mamba-ssm[causal-conv1d]\n",
        "# !pip install -q mamba-ssm[dev]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T09:56:58.674489Z",
          "iopub.status.busy": "2025-04-01T09:56:58.674226Z",
          "iopub.status.idle": "2025-04-01T09:57:02.161228Z",
          "shell.execute_reply": "2025-04-01T09:57:02.160117Z",
          "shell.execute_reply.started": "2025-04-01T09:56:58.674464Z"
        },
        "id": "Y0_o9pKb1Dc3",
        "outputId": "9fe2c8ea-3648-4c71-a056-219c8896e7fd",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: triton in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "CPU times: user 66.8 ms, sys: 15.8 ms, total: 82.6 ms\n",
            "Wall time: 3.48 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "!pip install triton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T09:57:02.162541Z",
          "iopub.status.busy": "2025-04-01T09:57:02.162217Z",
          "iopub.status.idle": "2025-04-01T09:57:49.652239Z",
          "shell.execute_reply": "2025-04-01T09:57:49.651155Z",
          "shell.execute_reply.started": "2025-04-01T09:57:02.162495Z"
        },
        "id": "7pEJ-MyA1Dc3",
        "outputId": "9c3040c4-1490-451b-fd2f-a7717efd1f60",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m818.9/818.9 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.2/71.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.4/101.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m529.7/529.7 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 4.25.6 which is incompatible.\n",
            "google-cloud-bigtable 2.27.0 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\n",
            "pandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
            "tensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCPU times: user 834 ms, sys: 190 ms, total: 1.02 s\n",
            "Wall time: 47.5 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "!pip -q install lightning torchscale evaluate huggingface_hub unbabel-comet flash-attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T09:57:49.653763Z",
          "iopub.status.busy": "2025-04-01T09:57:49.653413Z",
          "iopub.status.idle": "2025-04-01T09:57:54.286193Z",
          "shell.execute_reply": "2025-04-01T09:57:54.285261Z",
          "shell.execute_reply.started": "2025-04-01T09:57:49.653727Z"
        },
        "id": "jK2NoS4o1Dc3",
        "outputId": "ed954d5a-c3d0-431d-e33e-6ae799ca37e8",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.2/88.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.0/103.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCPU times: user 87.6 ms, sys: 26.4 ms, total: 114 ms\n",
            "Wall time: 4.63 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "!pip install -q x-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPQD4x5t1Dc3"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T09:57:54.287507Z",
          "iopub.status.busy": "2025-04-01T09:57:54.287188Z",
          "iopub.status.idle": "2025-04-01T09:57:54.598087Z",
          "shell.execute_reply": "2025-04-01T09:57:54.597373Z",
          "shell.execute_reply.started": "2025-04-01T09:57:54.287466Z"
        },
        "id": "x-PbCNpu1Dc3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import re\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hF5QH_iO1Dc3"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T09:57:54.601648Z",
          "iopub.status.busy": "2025-04-01T09:57:54.601165Z",
          "iopub.status.idle": "2025-04-01T09:57:55.78479Z",
          "shell.execute_reply": "2025-04-01T09:57:55.783855Z",
          "shell.execute_reply.started": "2025-04-01T09:57:54.601626Z"
        },
        "id": "76CghEmA1Dc3",
        "outputId": "1bef3f27-fb8d-4c2a-81ba-b34e7d85a2a1",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size: 12441, Validation size: 1555, Test size: 1556\n"
          ]
        }
      ],
      "source": [
        "def read_txt(path):\n",
        "    with open(path,'r', encoding='utf-8') as f:\n",
        "        lines = f.read().split('\\n')\n",
        "    text_pairs = []\n",
        "    mx = 0\n",
        "    mn = 100\n",
        "    x = []\n",
        "    for _line in lines:\n",
        "        arr = _line.split(':')\n",
        "        if len(arr) == 1:\n",
        "            continue\n",
        "        text_pairs.append({'Event Type': arr[0], \\\n",
        "                          'Feynman Diagram': arr[1],   \\\n",
        "                           'Amplitude': arr[-2],        \\\n",
        "                           'Squared Amplitude': arr[-1] \\\n",
        "                          })\n",
        "    return text_pairs\n",
        "\n",
        "final_pairs = [read_txt(\n",
        "            f'/kaggle/input/squared-amplitudes-test-data/SYMBA - Test Data/QED-2-to-2-diag-TreeLevel-{i}.txt')\n",
        "            for i in range(10)]\n",
        "final_pairs = [xx for x in final_pairs for xx in x]\n",
        "\n",
        "# df = pd.DataFrame(final_pairs,columns=['Event Type','Feynman Diagram', 'Amplitude', 'Squared Amplitude'],dtype=['str','str','str','str'])\n",
        "df = pd.DataFrame(final_pairs, columns=['Event Type', 'Feynman Diagram', 'Amplitude', 'Squared Amplitude'])\n",
        "df = df.astype({'Event Type': 'string', 'Feynman Diagram': 'string', 'Amplitude': 'string', 'Squared Amplitude': 'string'})\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, shuffle=True)\n",
        "\n",
        "print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_df)}\")\n",
        "\n",
        "train_df.to_csv('train.csv')\n",
        "val_df.to_csv('val.csv')\n",
        "test_df.to_csv('test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T09:57:55.786901Z",
          "iopub.status.busy": "2025-04-01T09:57:55.786546Z",
          "iopub.status.idle": "2025-04-01T09:57:55.790583Z",
          "shell.execute_reply": "2025-04-01T09:57:55.78975Z",
          "shell.execute_reply.started": "2025-04-01T09:57:55.786865Z"
        },
        "id": "qFKHlYsv1Dc3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "\n",
        "# # Define target column name - easy to change\n",
        "# target_column = 'Squared Amplitude'  # Change this to your actual target column name\n",
        "\n",
        "# # Get unique targets and shuffle them\n",
        "# unique_targets = df[target_column].unique()\n",
        "# np.random.seed(42)  # For reproducibility\n",
        "# np.random.shuffle(unique_targets)\n",
        "\n",
        "# # Calculate split points for 70/15/15 distribution\n",
        "# n_targets = len(unique_targets)\n",
        "# train_idx = int(0.7 * n_targets)\n",
        "# val_idx = int(0.85 * n_targets)\n",
        "\n",
        "# # Split targets into train/val/test groups\n",
        "# train_targets = unique_targets[:train_idx]\n",
        "# val_targets = unique_targets[train_idx:val_idx]\n",
        "# test_targets = unique_targets[val_idx:]\n",
        "\n",
        "# # Create the dataframes based on target groups\n",
        "# train_df = df[df[target_column].isin(train_targets)].sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "# val_df = df[df[target_column].isin(val_targets)].sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "# test_df = df[df[target_column].isin(test_targets)].sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# # Print statistics\n",
        "# print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_df)}\")\n",
        "# print(f\"Train targets: {len(train_targets)}, Val targets: {len(val_targets)}, Test targets: {len(test_targets)}\")\n",
        "\n",
        "# # Save the datasets\n",
        "# train_df.to_csv('train.csv')\n",
        "# val_df.to_csv('val.csv')\n",
        "# test_df.to_csv('test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su5F4W8a1Dc3"
      },
      "source": [
        "# Vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T09:57:55.791743Z",
          "iopub.status.busy": "2025-04-01T09:57:55.791458Z",
          "iopub.status.idle": "2025-04-01T09:57:55.9773Z",
          "shell.execute_reply": "2025-04-01T09:57:55.976396Z",
          "shell.execute_reply.started": "2025-04-01T09:57:55.791712Z"
        },
        "id": "tKg207GD1Dc3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def reporthook(t):\n",
        "    \"\"\"\n",
        "    https://github.com/tqdm/tqdm.\n",
        "    \"\"\"\n",
        "    last_b = [0]\n",
        "\n",
        "    def inner(b=1, bsize=1, tsize=None):\n",
        "        \"\"\"\n",
        "        b: int, optional\n",
        "        Number of blocks just transferred [default: 1].\n",
        "        bsize: int, optional\n",
        "        Size of each block (in tqdm units) [default: 1].\n",
        "        tsize: int, optional\n",
        "        Total size (in tqdm units). If [default: None] remains unchanged.\n",
        "        \"\"\"\n",
        "        if tsize is not None:\n",
        "            t.total = tsize\n",
        "        t.update((b - last_b[0]) * bsize)\n",
        "        last_b[0] = b\n",
        "\n",
        "    return inner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T09:57:55.978481Z",
          "iopub.status.busy": "2025-04-01T09:57:55.978177Z",
          "iopub.status.idle": "2025-04-01T09:57:56.011764Z",
          "shell.execute_reply": "2025-04-01T09:57:56.010988Z",
          "shell.execute_reply.started": "2025-04-01T09:57:55.97845Z"
        },
        "id": "_A8r7Ujt1Dc4",
        "outputId": "f889e77c-0dcb-4a3b-b507-204e423394b4",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Mapping from string name to factory function'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import logging\n",
        "import os\n",
        "import zipfile\n",
        "import gzip\n",
        "from urllib.request import urlretrieve\n",
        "from tqdm import tqdm\n",
        "import tarfile\n",
        "from functools import partial\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def _infer_shape(f):\n",
        "    num_lines, vector_dim = 0, None\n",
        "    for line in f:\n",
        "        if vector_dim is None:\n",
        "            row = line.rstrip().split(b\" \")\n",
        "            vector = row[1:]\n",
        "            # Assuming word, [vector] format\n",
        "            if len(vector) > 2:\n",
        "                # The header present in some (w2v) formats contains two elements.\n",
        "                vector_dim = len(vector)\n",
        "                num_lines += 1  # First element read\n",
        "        else:\n",
        "            num_lines += 1\n",
        "    f.seek(0)\n",
        "    return num_lines, vector_dim\n",
        "\n",
        "\n",
        "class Vectors(object):\n",
        "\n",
        "    def __init__(self, name, cache=None,\n",
        "                 url=None, unk_init=None, max_vectors=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "\n",
        "            name: name of the file that contains the vectors\n",
        "            cache: directory for cached vectors\n",
        "            url: url for download if vectors not found in cache\n",
        "            unk_init (callback): by default, initialize out-of-vocabulary word vectors\n",
        "                to zero vectors; can be any function that takes in a Tensor and returns a Tensor of the same size\n",
        "            max_vectors (int): this can be used to limit the number of\n",
        "                pre-trained vectors loaded.\n",
        "                Most pre-trained vector sets are sorted\n",
        "                in the descending order of word frequency.\n",
        "                Thus, in situations where the entire set doesn't fit in memory,\n",
        "                or is not needed for another reason, passing `max_vectors`\n",
        "                can limit the size of the loaded set.\n",
        "        \"\"\"\n",
        "\n",
        "        cache = '.vector_cache' if cache is None else cache\n",
        "        self.itos = None\n",
        "        self.stoi = None\n",
        "        self.vectors = None\n",
        "        self.dim = None\n",
        "        self.unk_init = torch.Tensor.zero_ if unk_init is None else unk_init\n",
        "        self.cache(name, cache, url=url, max_vectors=max_vectors)\n",
        "\n",
        "    def __getitem__(self, token):\n",
        "        if token in self.stoi:\n",
        "            return self.vectors[self.stoi[token]]\n",
        "        else:\n",
        "            return self.unk_init(torch.Tensor(self.dim))\n",
        "\n",
        "    def cache(self, name, cache, url=None, max_vectors=None):\n",
        "        import ssl\n",
        "        ssl._create_default_https_context = ssl._create_unverified_context\n",
        "        if os.path.isfile(name):\n",
        "            path = name\n",
        "            if max_vectors:\n",
        "                file_suffix = '_{}.pt'.format(max_vectors)\n",
        "            else:\n",
        "                file_suffix = '.pt'\n",
        "            path_pt = os.path.join(cache, os.path.basename(name)) + file_suffix\n",
        "        else:\n",
        "            path = os.path.join(cache, name)\n",
        "            if max_vectors:\n",
        "                file_suffix = '_{}.pt'.format(max_vectors)\n",
        "            else:\n",
        "                file_suffix = '.pt'\n",
        "            path_pt = path + file_suffix\n",
        "\n",
        "        if not os.path.isfile(path_pt):\n",
        "            if not os.path.isfile(path) and url:\n",
        "                logger.info('Downloading vectors from {}'.format(url))\n",
        "                if not os.path.exists(cache):\n",
        "                    os.makedirs(cache)\n",
        "                dest = os.path.join(cache, os.path.basename(url))\n",
        "                if not os.path.isfile(dest):\n",
        "                    with tqdm(unit='B', unit_scale=True, miniters=1, desc=dest) as t:\n",
        "                        try:\n",
        "                            urlretrieve(url, dest, reporthook=reporthook(t))\n",
        "                        except KeyboardInterrupt as e:  # remove the partial zip file\n",
        "                            os.remove(dest)\n",
        "                            raise e\n",
        "                logger.info('Extracting vectors into {}'.format(cache))\n",
        "                ext = os.path.splitext(dest)[1][1:]\n",
        "                if ext == 'zip':\n",
        "                    with zipfile.ZipFile(dest, \"r\") as zf:\n",
        "                        zf.extractall(cache)\n",
        "                elif ext == 'gz':\n",
        "                    if dest.endswith('.tar.gz'):\n",
        "                        with tarfile.open(dest, 'r:gz') as tar:\n",
        "                            tar.extractall(path=cache)\n",
        "            if not os.path.isfile(path):\n",
        "                raise RuntimeError('no vectors found at {}'.format(path))\n",
        "\n",
        "            logger.info(\"Loading vectors from {}\".format(path))\n",
        "            ext = os.path.splitext(path)[1][1:]\n",
        "            if ext == 'gz':\n",
        "                open_file = gzip.open\n",
        "            else:\n",
        "                open_file = open\n",
        "\n",
        "            vectors_loaded = 0\n",
        "            with open_file(path, 'rb') as f:\n",
        "                num_lines, dim = _infer_shape(f)\n",
        "                if not max_vectors or max_vectors > num_lines:\n",
        "                    max_vectors = num_lines\n",
        "\n",
        "                itos, vectors, dim = [], torch.zeros((max_vectors, dim)), None\n",
        "\n",
        "                for line in tqdm(f, total=max_vectors):\n",
        "                    # Explicitly splitting on \" \" is important, so we don't\n",
        "                    # get rid of Unicode non-breaking spaces in the vectors.\n",
        "                    entries = line.rstrip().split(b\" \")\n",
        "\n",
        "                    word, entries = entries[0], entries[1:]\n",
        "                    if dim is None and len(entries) > 1:\n",
        "                        dim = len(entries)\n",
        "                    elif len(entries) == 1:\n",
        "                        logger.warning(\"Skipping token {} with 1-dimensional \"\n",
        "                                       \"vector {}; likely a header\".format(word, entries))\n",
        "                        continue\n",
        "                    elif dim != len(entries):\n",
        "                        raise RuntimeError(\n",
        "                            \"Vector for token {} has {} dimensions, but previously \"\n",
        "                            \"read vectors have {} dimensions. All vectors must have \"\n",
        "                            \"the same number of dimensions.\".format(word, len(entries),\n",
        "                                                                    dim))\n",
        "\n",
        "                    try:\n",
        "                        if isinstance(word, bytes):\n",
        "                            word = word.decode('utf-8')\n",
        "                    except UnicodeDecodeError:\n",
        "                        logger.info(\"Skipping non-UTF8 token {}\".format(repr(word)))\n",
        "                        continue\n",
        "\n",
        "                    vectors[vectors_loaded] = torch.tensor([float(x) for x in entries])\n",
        "                    vectors_loaded += 1\n",
        "                    itos.append(word)\n",
        "\n",
        "                    if vectors_loaded == max_vectors:\n",
        "                        break\n",
        "\n",
        "            self.itos = itos\n",
        "            self.stoi = {word: i for i, word in enumerate(itos)}\n",
        "            self.vectors = torch.Tensor(vectors).view(-1, dim)\n",
        "            self.dim = dim\n",
        "            logger.info('Saving vectors to {}'.format(path_pt))\n",
        "            if not os.path.exists(cache):\n",
        "                os.makedirs(cache)\n",
        "            torch.save((self.itos, self.stoi, self.vectors, self.dim), path_pt)\n",
        "        else:\n",
        "            logger.info('Loading vectors from {}'.format(path_pt))\n",
        "            self.itos, self.stoi, self.vectors, self.dim = torch.load(path_pt)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vectors)\n",
        "\n",
        "    def get_vecs_by_tokens(self, tokens, lower_case_backup=False):\n",
        "        \"\"\"Look up embedding vectors of tokens.\n",
        "\n",
        "        Args:\n",
        "            tokens: a token or a list of tokens. if `tokens` is a string,\n",
        "                returns a 1-D tensor of shape `self.dim`; if `tokens` is a\n",
        "                list of strings, returns a 2-D tensor of shape=(len(tokens),\n",
        "                self.dim).\n",
        "            lower_case_backup : Whether to look up the token in the lower case.\n",
        "                If False, each token in the original case will be looked up;\n",
        "                if True, each token in the original case will be looked up first,\n",
        "                if not found in the keys of the property `stoi`, the token in the\n",
        "                lower case will be looked up. Default: False.\n",
        "\n",
        "        Examples:\n",
        "            >>> examples = ['chip', 'baby', 'Beautiful']\n",
        "            >>> vec = text.vocab.GloVe(name='6B', dim=50)\n",
        "            >>> ret = vec.get_vecs_by_tokens(tokens, lower_case_backup=True)\n",
        "        \"\"\"\n",
        "        to_reduce = False\n",
        "\n",
        "        if not isinstance(tokens, list):\n",
        "            tokens = [tokens]\n",
        "            to_reduce = True\n",
        "\n",
        "        if not lower_case_backup:\n",
        "            indices = [self[token] for token in tokens]\n",
        "        else:\n",
        "            indices = [self[token] if token in self.stoi\n",
        "                       else self[token.lower()]\n",
        "                       for token in tokens]\n",
        "\n",
        "        vecs = torch.stack(indices)\n",
        "        return vecs[0] if to_reduce else vecs\n",
        "\n",
        "\n",
        "class GloVe(Vectors):\n",
        "    url = {\n",
        "        '42B': 'http://nlp.stanford.edu/data/glove.42B.300d.zip',\n",
        "        '840B': 'http://nlp.stanford.edu/data/glove.840B.300d.zip',\n",
        "        'twitter.27B': 'http://nlp.stanford.edu/data/glove.twitter.27B.zip',\n",
        "        '6B': 'http://nlp.stanford.edu/data/glove.6B.zip',\n",
        "    }\n",
        "\n",
        "    def __init__(self, name='840B', dim=300, **kwargs):\n",
        "        url = self.url[name]\n",
        "        name = 'glove.{}.{}d.txt'.format(name, str(dim))\n",
        "        super(GloVe, self).__init__(name, url=url, **kwargs)\n",
        "\n",
        "\n",
        "class FastText(Vectors):\n",
        "\n",
        "    url_base = 'https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.{}.vec'\n",
        "\n",
        "    def __init__(self, language=\"en\", **kwargs):\n",
        "        url = self.url_base.format(language)\n",
        "        name = os.path.basename(url)\n",
        "        super(FastText, self).__init__(name, url=url, **kwargs)\n",
        "\n",
        "\n",
        "class CharNGram(Vectors):\n",
        "\n",
        "    name = 'charNgram.txt'\n",
        "    url = ('http://www.logos.t.u-tokyo.ac.jp/~hassy/publications/arxiv2016jmt/'\n",
        "           'jmt_pre-trained_embeddings.tar.gz')\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(CharNGram, self).__init__(self.name, url=self.url, **kwargs)\n",
        "\n",
        "    def __getitem__(self, token):\n",
        "        vector = torch.Tensor(1, self.dim).zero_()\n",
        "        if token == \"<unk>\":\n",
        "            return self.unk_init(vector)\n",
        "        chars = ['#BEGIN#'] + list(token) + ['#END#']\n",
        "        num_vectors = 0\n",
        "        for n in [2, 3, 4]:\n",
        "            end = len(chars) - n + 1\n",
        "            grams = [chars[i:(i + n)] for i in range(end)]\n",
        "            for gram in grams:\n",
        "                gram_key = '{}gram-{}'.format(n, ''.join(gram))\n",
        "                if gram_key in self.stoi:\n",
        "                    vector += self.vectors[self.stoi[gram_key]]\n",
        "                    num_vectors += 1\n",
        "        if num_vectors > 0:\n",
        "            vector /= num_vectors\n",
        "        else:\n",
        "            vector = self.unk_init(vector)\n",
        "        return vector\n",
        "\n",
        "\n",
        "pretrained_aliases = {\n",
        "    \"charngram.100d\": partial(CharNGram),\n",
        "    \"fasttext.en.300d\": partial(FastText, language=\"en\"),\n",
        "    \"fasttext.simple.300d\": partial(FastText, language=\"simple\"),\n",
        "    \"glove.42B.300d\": partial(GloVe, name=\"42B\", dim=\"300\"),\n",
        "    \"glove.840B.300d\": partial(GloVe, name=\"840B\", dim=\"300\"),\n",
        "    \"glove.twitter.27B.25d\": partial(GloVe, name=\"twitter.27B\", dim=\"25\"),\n",
        "    \"glove.twitter.27B.50d\": partial(GloVe, name=\"twitter.27B\", dim=\"50\"),\n",
        "    \"glove.twitter.27B.100d\": partial(GloVe, name=\"twitter.27B\", dim=\"100\"),\n",
        "    \"glove.twitter.27B.200d\": partial(GloVe, name=\"twitter.27B\", dim=\"200\"),\n",
        "    \"glove.6B.50d\": partial(GloVe, name=\"6B\", dim=\"50\"),\n",
        "    \"glove.6B.100d\": partial(GloVe, name=\"6B\", dim=\"100\"),\n",
        "    \"glove.6B.200d\": partial(GloVe, name=\"6B\", dim=\"200\"),\n",
        "    \"glove.6B.300d\": partial(GloVe, name=\"6B\", dim=\"300\")\n",
        "}\n",
        "\"\"\"Mapping from string name to factory function\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T09:57:56.012962Z",
          "iopub.status.busy": "2025-04-01T09:57:56.012706Z",
          "iopub.status.idle": "2025-04-01T09:57:56.037013Z",
          "shell.execute_reply": "2025-04-01T09:57:56.036153Z",
          "shell.execute_reply.started": "2025-04-01T09:57:56.012942Z"
        },
        "id": "zlBlx_8x1Dc4",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import logging\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "# from torchtext.vocab import (\n",
        "#     pretrained_aliases,  # not in legacy\n",
        "#     Vectors,  # not in legacy\n",
        "# )\n",
        "from typing import List\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class Vocab(object):\n",
        "    \"\"\"Defines a vocabulary object that will be used to numericalize a field.\n",
        "\n",
        "    Attributes:\n",
        "        freqs: A collections.Counter object holding the frequencies of tokens\n",
        "            in the data used to build the Vocab.\n",
        "        stoi: A collections.defaultdict instance mapping token strings to\n",
        "            numerical identifiers.\n",
        "        itos: A list of token strings indexed by their numerical identifiers.\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO (@mttk): Populate classs with default values of special symbols\n",
        "    UNK = '<unk>'\n",
        "\n",
        "    def __init__(self, counter, max_size=None, min_freq=1, specials=('<unk>', '<pad>'),\n",
        "                 vectors=None, unk_init=None, vectors_cache=None, specials_first=True):\n",
        "        \"\"\"Create a Vocab object from a collections.Counter.\n",
        "\n",
        "        Args:\n",
        "            counter: collections.Counter object holding the frequencies of\n",
        "                each value found in the data.\n",
        "            max_size: The maximum size of the vocabulary, or None for no\n",
        "                maximum. Default: None.\n",
        "            min_freq: The minimum frequency needed to include a token in the\n",
        "                vocabulary. Values less than 1 will be set to 1. Default: 1.\n",
        "            specials: The list of special tokens (e.g., padding or eos) that\n",
        "                will be prepended to the vocabulary. Default: ['<unk'>, '<pad>']\n",
        "            vectors: One of either the available pretrained vectors\n",
        "                or custom pretrained vectors (see Vocab.load_vectors);\n",
        "                or a list of aforementioned vectors\n",
        "            unk_init (callback): by default, initialize out-of-vocabulary word vectors\n",
        "                to zero vectors; can be any function that takes in a Tensor and\n",
        "                returns a Tensor of the same size. Default: 'torch.zeros'\n",
        "            vectors_cache: directory for cached vectors. Default: '.vector_cache'\n",
        "            specials_first: Whether to add special tokens into the vocabulary at first.\n",
        "                If it is False, they are added into the vocabulary at last.\n",
        "                Default: True.\n",
        "        \"\"\"\n",
        "        self.freqs = counter\n",
        "        counter = counter.copy()\n",
        "        min_freq = max(min_freq, 1)\n",
        "\n",
        "        self.itos = list()\n",
        "        self.unk_index = None\n",
        "        if specials_first:\n",
        "            self.itos = list(specials)\n",
        "            # only extend max size if specials are prepended\n",
        "            max_size = None if max_size is None else max_size + len(specials)\n",
        "\n",
        "        # frequencies of special tokens are not counted when building vocabulary\n",
        "        # in frequency order\n",
        "        for tok in specials:\n",
        "            if tok in counter:\n",
        "                del counter[tok]\n",
        "\n",
        "        # sort by frequency, then alphabetically\n",
        "        words_and_frequencies = sorted(counter.items(), key=lambda tup: tup[0])\n",
        "        words_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)\n",
        "\n",
        "        for word, freq in words_and_frequencies:\n",
        "            if freq < min_freq or len(self.itos) == max_size:\n",
        "                break\n",
        "            self.itos.append(word)\n",
        "\n",
        "        if Vocab.UNK in specials:  # hard-coded for now\n",
        "            unk_index = specials.index(Vocab.UNK)  # position in list\n",
        "            # account for ordering of specials, set variable\n",
        "            self.unk_index = unk_index if specials_first else len(self.itos) + unk_index\n",
        "            self.stoi = defaultdict(self._default_unk_index)\n",
        "        else:\n",
        "            self.stoi = defaultdict()\n",
        "\n",
        "        if not specials_first:\n",
        "            self.itos.extend(list(specials))\n",
        "\n",
        "        # stoi is simply a reverse dict for itos\n",
        "        self.stoi.update({tok: i for i, tok in enumerate(self.itos)})\n",
        "\n",
        "        self.vectors = None\n",
        "        if vectors is not None:\n",
        "            self.load_vectors(vectors, unk_init=unk_init, cache=vectors_cache)\n",
        "        else:\n",
        "            assert unk_init is None and vectors_cache is None\n",
        "\n",
        "    def forward(self, tokens: List[str]) -> List[int]:\n",
        "        return self.lookup_indices(tokens)\n",
        "\n",
        "    def set_default_index(self, idx):\n",
        "        self.unk_index = idx\n",
        "\n",
        "    def _default_unk_index(self):\n",
        "        return self.unk_index\n",
        "\n",
        "    def __getitem__(self, token):\n",
        "        return self.stoi.get(token, self.stoi.get(Vocab.UNK))\n",
        "\n",
        "    def __getstate__(self):\n",
        "        # avoid picking defaultdict\n",
        "        attrs = dict(self.__dict__)\n",
        "        # cast to regular dict\n",
        "        attrs['stoi'] = dict(self.stoi)\n",
        "        return attrs\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        if state.get(\"unk_index\", None) is None:\n",
        "            stoi = defaultdict()\n",
        "        else:\n",
        "            stoi = defaultdict(self._default_unk_index)\n",
        "        stoi.update(state['stoi'])\n",
        "        state['stoi'] = stoi\n",
        "        self.__dict__.update(state)\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        if self.freqs != other.freqs:\n",
        "            return False\n",
        "        if self.stoi != other.stoi:\n",
        "            return False\n",
        "        if self.itos != other.itos:\n",
        "            return False\n",
        "        if self.vectors != other.vectors:\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    def lookup_indices(self, tokens):\n",
        "        indices = [self.__getitem__(token) for token in tokens]\n",
        "        return indices\n",
        "\n",
        "    def extend(self, v, sort=False):\n",
        "        words = sorted(v.itos) if sort else v.itos\n",
        "        for w in words:\n",
        "            if w not in self.stoi:\n",
        "                self.itos.append(w)\n",
        "                self.stoi[w] = len(self.itos) - 1\n",
        "\n",
        "    def load_vectors(self, vectors, **kwargs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            vectors: one of or a list containing instantiations of the\n",
        "                GloVe, CharNGram, or Vectors classes. Alternatively, one\n",
        "                of or a list of available pretrained vectors:\n",
        "\n",
        "                charngram.100d\n",
        "                fasttext.en.300d\n",
        "                fasttext.simple.300d\n",
        "                glove.42B.300d\n",
        "                glove.840B.300d\n",
        "                glove.twitter.27B.25d\n",
        "                glove.twitter.27B.50d\n",
        "                glove.twitter.27B.100d\n",
        "                glove.twitter.27B.200d\n",
        "                glove.6B.50d\n",
        "                glove.6B.100d\n",
        "                glove.6B.200d\n",
        "                glove.6B.300d\n",
        "\n",
        "            Remaining keyword arguments: Passed to the constructor of Vectors classes.\n",
        "        \"\"\"\n",
        "        if not isinstance(vectors, list):\n",
        "            vectors = [vectors]\n",
        "        for idx, vector in enumerate(vectors):\n",
        "            if isinstance(vector, str):\n",
        "                # Convert the string pretrained vector identifier\n",
        "                # to a Vectors object\n",
        "                if vector not in pretrained_aliases:\n",
        "                    raise ValueError(\"Got string input vector {}, but allowed pretrained vectors are {}\".format(vector, list(pretrained_aliases.keys())))\n",
        "                vectors[idx] = pretrained_aliases[vector](**kwargs)\n",
        "            elif not isinstance(vector, Vectors):\n",
        "                raise ValueError( \"Got input vectors of type {}, expected str or Vectors object\".format(type(vector)))\n",
        "        tot_dim = sum(v.dim for v in vectors)\n",
        "        self.vectors = torch.Tensor(len(self), tot_dim)\n",
        "        for i, token in enumerate(self.itos):\n",
        "            start_dim = 0\n",
        "            for v in vectors:\n",
        "                end_dim = start_dim + v.dim\n",
        "                self.vectors[i][start_dim:end_dim] = v[token.strip()]\n",
        "                start_dim = end_dim\n",
        "            assert(start_dim == tot_dim)\n",
        "\n",
        "    def set_vectors(self, stoi, vectors, dim, unk_init=torch.Tensor.zero_):\n",
        "        \"\"\"\n",
        "        Set the vectors for the Vocab instance from a collection of Tensors.\n",
        "\n",
        "        Args:\n",
        "            stoi: A dictionary of string to the index of the associated vector\n",
        "                in the `vectors` input argument.\n",
        "            vectors: An indexed iterable (or other structure supporting __getitem__) that\n",
        "                given an input index, returns a FloatTensor representing the vector\n",
        "                for the token associated with the index. For example,\n",
        "                vector[stoi[\"string\"]] should return the vector for \"string\".\n",
        "            dim: The dimensionality of the vectors.\n",
        "            unk_init (callback): by default, initialize out-of-vocabulary word vectors\n",
        "                to zero vectors; can be any function that takes in a Tensor and\n",
        "                returns a Tensor of the same size. Default: 'torch.zeros'\n",
        "        \"\"\"\n",
        "        self.vectors = torch.Tensor(len(self), dim)\n",
        "        for i, token in enumerate(self.itos):\n",
        "            wv_index = stoi.get(token, None)\n",
        "            if wv_index is not None:\n",
        "                self.vectors[i] = vectors[wv_index]\n",
        "            else:\n",
        "                self.vectors[i] = unk_init(self.vectors[i])\n",
        "\n",
        "\n",
        "class SubwordVocab(Vocab):\n",
        "\n",
        "    def __init__(self, counter, max_size=None, specials=('<pad>'),\n",
        "                 vectors=None, unk_init=torch.Tensor.zero_):\n",
        "        \"\"\"Create a revtok subword vocabulary from a collections.Counter.\n",
        "\n",
        "        Args:\n",
        "            counter: collections.Counter object holding the frequencies of\n",
        "                each word found in the data.\n",
        "            max_size: The maximum size of the subword vocabulary, or None for no\n",
        "                maximum. Default: None.\n",
        "            specials: The list of special tokens (e.g., padding or eos) that\n",
        "                will be prepended to the vocabulary in addition to an <unk>\n",
        "                token.\n",
        "            vectors: One of either the available pretrained vectors\n",
        "                or custom pretrained vectors (see Vocab.load_vectors);\n",
        "                or a list of aforementioned vectors\n",
        "            unk_init (callback): by default, initialize out-of-vocabulary word vectors\n",
        "                to zero vectors; can be any function that takes in a Tensor and\n",
        "                returns a Tensor of the same size. Default: 'torch.zeros\n",
        "        \"\"\"\n",
        "        try:\n",
        "            import revtok\n",
        "        except ImportError:\n",
        "            print(\"Please install revtok.\")\n",
        "            raise\n",
        "\n",
        "        # Hardcode unk_index as subword_vocab has no specials_first argument\n",
        "        self.unk_index = (specials.index(SubwordVocab.UNK)\n",
        "                          if SubwordVocab.UNK in specials else None)\n",
        "\n",
        "        if self.unk_index is None:\n",
        "            self.stoi = defaultdict()\n",
        "        else:\n",
        "            self.stoi = defaultdict(self._default_unk_index)\n",
        "\n",
        "        self.stoi.update({tok: i for i, tok in enumerate(specials)})\n",
        "        self.itos = specials.copy()\n",
        "\n",
        "        self.segment = revtok.SubwordSegmenter(counter, max_size)\n",
        "\n",
        "        max_size = None if max_size is None else max_size + len(self.itos)\n",
        "\n",
        "        # sort by frequency/entropy, then alphabetically\n",
        "        toks = sorted(self.segment.vocab.items(), key=lambda tup: (len(tup[0]) != 1, -tup[1], tup[0]))\n",
        "\n",
        "        for tok, _ in toks:\n",
        "            if len(self.itos) == max_size:\n",
        "                break\n",
        "            self.itos.append(tok)\n",
        "            self.stoi[tok] = len(self.itos) - 1\n",
        "\n",
        "        if vectors is not None:\n",
        "            self.load_vectors(vectors, unk_init=unk_init)\n",
        "\n",
        "\n",
        "def build_vocab_from_iterator(iterator, num_lines=None):\n",
        "    \"\"\"\n",
        "    Build a Vocab from an iterator.\n",
        "\n",
        "    Args:\n",
        "        iterator: Iterator used to build Vocab. Must yield list or iterator of tokens.\n",
        "        num_lines: The expected number of elements returned by the iterator.\n",
        "            (Default: None)\n",
        "            Optionally, if known, the expected number of elements can be passed to\n",
        "            this factory function for improved progress reporting.\n",
        "    \"\"\"\n",
        "\n",
        "    counter = Counter()\n",
        "    with tqdm(unit_scale=0, unit='lines', total=num_lines) as t:\n",
        "        for tokens in iterator:\n",
        "            counter.update(tokens)\n",
        "            t.update(1)\n",
        "    word_vocab = Vocab(counter)\n",
        "    return word_vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCyrfyG31Dc4"
      },
      "source": [
        "# Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T09:57:56.038397Z",
          "iopub.status.busy": "2025-04-01T09:57:56.037887Z",
          "iopub.status.idle": "2025-04-01T09:57:56.06193Z",
          "shell.execute_reply": "2025-04-01T09:57:56.061063Z",
          "shell.execute_reply.started": "2025-04-01T09:57:56.038354Z"
        },
        "id": "4KtRz2rf1Dc4",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from collections import Counter, OrderedDict\n",
        "from itertools import cycle\n",
        "import re\n",
        "import random\n",
        "# from torchtext.vocab import vocab # Built it custom\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "\n",
        "class Tokenizer:\n",
        "    \"\"\"\n",
        "    Tokenizer for processing symbolic mathematical expressions.\n",
        "    \"\"\"\n",
        "    def __init__(self, df, index_token_pool_size, momentum_token_pool_size, special_symbols, UNK_IDX, to_replace):\n",
        "        self.amps = df.Amplitude.tolist()\n",
        "        self.sqamps = df['Squared Amplitude'].tolist()\n",
        "\n",
        "        # Issue warnings if token pool sizes are too small\n",
        "        if index_token_pool_size < 100:\n",
        "            warnings.warn(f\"Index token pool size ({index_token_pool_size}) is small. Consider increasing it.\", UserWarning)\n",
        "        if momentum_token_pool_size < 100:\n",
        "            warnings.warn(f\"Momentum token pool size ({momentum_token_pool_size}) is small. Consider increasing it.\", UserWarning)\n",
        "\n",
        "        # Generate token pools\n",
        "        self.tokens_pool = [f\"_{i}\" for i in range(index_token_pool_size)]\n",
        "        self.momentum_pool = [f\"MOMENTUM_{i}\" for i in range(momentum_token_pool_size)]\n",
        "\n",
        "        # Regular expression patterns for token replacement\n",
        "        self.pattern_momentum = re.compile(r'\\b[p]_\\d{1,}\\b')\n",
        "        self.pattern_num_123 = re.compile(r'\\b(?![ps]_)\\w+_\\d{1,}\\b')\n",
        "        self.pattern_special = re.compile(r'\\b\\w+_+\\w+\\b\\\\')\n",
        "        self.pattern_prop = re.compile(r'Prop')\n",
        "        self.pattern_int = re.compile(r'int\\{')\n",
        "        self.pattern_operators = {\n",
        "            '+': re.compile(r'\\+'), '-': re.compile(r'-'), '*': re.compile(r'\\*'),\n",
        "            ',': re.compile(r','), '^': re.compile(r'\\^'), '%': re.compile(r'%'),\n",
        "            '}': re.compile(r'\\}'), '(': re.compile(r'\\('), ')': re.compile(r'\\)')\n",
        "        }\n",
        "        self.function_opening = re.compile(r'(\\w+_\\{)')\n",
        "        self.pattern_mass = re.compile(r'\\b\\w+_\\w\\b')\n",
        "        self.pattern_s = re.compile(r'\\b\\w+_\\d{2,}\\b')\n",
        "        self.pattern_reg_prop = re.compile(r'\\b\\w+_\\d{1}\\b')\n",
        "        self.pattern_antipart = re.compile(r'(\\w)_\\w+_\\d+\\(X\\)\\^\\(\\*\\)')\n",
        "        self.pattern_part = re.compile(r'(\\w)_\\w+_\\d+\\(X\\)')\n",
        "        self.pattern_index = re.compile(r'\\b\\w+_\\w+_\\d{2,}\\b')\n",
        "\n",
        "        self.special_symbols = special_symbols\n",
        "        self.UNK_IDX = UNK_IDX\n",
        "        self.to_replace = to_replace\n",
        "\n",
        "        self.bos_token_id = 0\n",
        "        self.eos_token_id = 2\n",
        "        self.pad_token_id = 1\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_whitespace(expression):\n",
        "        \"\"\"Remove all forms of whitespace from the expression.\"\"\"\n",
        "        return re.sub(r'\\s+', '', expression)\n",
        "\n",
        "    @staticmethod\n",
        "    def split_expression(expression):\n",
        "        \"\"\"Split the expression by space delimiter.\"\"\"\n",
        "        return re.split(r' ', expression)\n",
        "\n",
        "    def build_tgt_vocab(self):\n",
        "        \"\"\"Build vocabulary for target sequences.\"\"\"\n",
        "        counter = Counter()\n",
        "        for eqn in tqdm(self.sqamps, desc='Processing target vocab'):\n",
        "            counter.update(self.tgt_tokenize(eqn))\n",
        "        voc = Vocab(OrderedDict(counter), specials=self.special_symbols[:], specials_first=True)\n",
        "        voc.set_default_index(self.UNK_IDX)\n",
        "        return voc\n",
        "\n",
        "    def build_src_vocab(self, seed):\n",
        "        \"\"\"Build vocabulary for source sequences.\"\"\"\n",
        "        counter = Counter()\n",
        "        for diag in tqdm(self.amps, desc='Processing source vocab'):\n",
        "            counter.update(self.src_tokenize(diag, seed))\n",
        "        voc = Vocab(OrderedDict(counter), specials=self.special_symbols[:], specials_first=True)\n",
        "        voc.set_default_index(self.UNK_IDX)\n",
        "        return voc\n",
        "\n",
        "    def src_replace(self, ampl, seed):\n",
        "        \"\"\"Replace indexed and momentum variables with tokenized equivalents.\"\"\"\n",
        "        ampl = self.remove_whitespace(ampl)\n",
        "\n",
        "        random.seed(seed)\n",
        "        token_cycle = cycle(random.sample(self.tokens_pool, len(self.tokens_pool)))\n",
        "        momentum_cycle = cycle(random.sample(self.momentum_pool, len(self.momentum_pool)))\n",
        "\n",
        "        # Replace momentum tokens\n",
        "        temp_ampl = ampl\n",
        "        momentum_mapping = {match: next(momentum_cycle) for match in set(self.pattern_momentum.findall(ampl))}\n",
        "        for key, value in momentum_mapping.items():\n",
        "            temp_ampl = temp_ampl.replace(key, value)\n",
        "\n",
        "\n",
        "        def replace_123_match(match):\n",
        "            word, num = match.group().rsplit('_', 1)\n",
        "            # if word == 's':\n",
        "            #     # Mandstein\n",
        "            #     return match.group()\n",
        "            # if word == 'p':\n",
        "            #     # return match.group()  # Keep 'p_X' unchanged\n",
        "            #     return f'MOMENTUM_{num}'\n",
        "            # IDX_POOL.setdefault(num, len(IDX_POOL) + 1)\n",
        "            return f\"{word}_INDEX_{next(token_cycle)}\"\n",
        "        # Replace index tokens\n",
        "        # num_123_mapping = {match: match.rsplit('_',1) + next(token_cycle) for match in set(self.pattern_num_123.findall(ampl))}\n",
        "        # for key, value in num_123_mapping.items():\n",
        "        #     temp_ampl = temp_ampl.replace(key, value)\n",
        "        temp_ampl = re.sub(self.pattern_num_123,replace_123_match,ampl)\n",
        "        return temp_ampl\n",
        "\n",
        "    def src_tokenize(self, ampl, seed):\n",
        "        \"\"\"Tokenize source expression, optionally applying replacements.\"\"\"\n",
        "        temp_ampl = self.src_replace(ampl, seed) if self.to_replace else ampl\n",
        "        temp_ampl = temp_ampl.replace('\\\\\\\\', '\\\\').replace('\\\\', ' \\\\ ').replace('%', '')\n",
        "\n",
        "        for symbol, pattern in self.pattern_operators.items():\n",
        "            temp_ampl = pattern.sub(f' {symbol} ', temp_ampl)\n",
        "\n",
        "        temp_ampl = self.function_opening.sub(r'\\1 ', temp_ampl)\n",
        "\n",
        "        temp_ampl = re.sub(r' {2,}', ' ', temp_ampl)\n",
        "        return [token for token in self.split_expression(temp_ampl) if token]\n",
        "\n",
        "    def tgt_tokenize(self, sqampl):\n",
        "        \"\"\"Tokenize target expression.\"\"\"\n",
        "        sqampl = self.remove_whitespace(sqampl)\n",
        "        temp_sqampl = sqampl\n",
        "\n",
        "        for symbol, pattern in self.pattern_operators.items():\n",
        "            temp_sqampl = pattern.sub(f' {symbol} ', temp_sqampl)\n",
        "\n",
        "        for pattern in [self.pattern_reg_prop, self.pattern_mass, self.pattern_s]:\n",
        "            temp_sqampl = pattern.sub(lambda match: f' {match.group(0)} ', temp_sqampl)\n",
        "\n",
        "        temp_sqampl = re.sub(r' {2,}', ' ', temp_sqampl)\n",
        "        return [token for token in self.split_expression(temp_sqampl) if token]\n",
        "\n",
        "    def batch_decode(self, batch_of_tokens, vocab):\n",
        "        \"\"\"\n",
        "        Decode a batch of token sequences back to their original text form.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        batch_of_tokens : List[List[int]]\n",
        "            A batch of token sequences, where each sequence is a list of token indices\n",
        "        vocab : Vocab\n",
        "            The vocabulary object used for mapping indices to tokens\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        List[str]\n",
        "            The decoded expressions\n",
        "        \"\"\"\n",
        "        decoded_expressions = []\n",
        "\n",
        "        for token_sequence in batch_of_tokens:\n",
        "            # Convert indices to tokens\n",
        "            tokens = [vocab.itos[idx] for idx in token_sequence if idx != self.UNK_IDX]\n",
        "\n",
        "            # # Replace special tokens with their original form\n",
        "            # for i, token in enumerate(tokens):\n",
        "            #     if token in self.token_to_original:\n",
        "            #         tokens[i] = self.token_to_original[token]\n",
        "\n",
        "            # Remove special symbols that might have been added\n",
        "            tokens = [t for t in tokens if t not in self.special_symbols]\n",
        "\n",
        "            # Join tokens into a string and remove excess spaces\n",
        "            decoded_expr = ''.join(tokens)\n",
        "\n",
        "            # Clean up the expression by removing spaces around operators\n",
        "            for symbol in self.pattern_operators.keys():\n",
        "                decoded_expr = decoded_expr.replace(f\" {symbol} \", symbol)\n",
        "\n",
        "            # # Further cleaning for specific patterns\n",
        "            # decoded_expr = decoded_expr.replace('\\\\', '').replace(' ', '')\n",
        "\n",
        "            decoded_expressions.append(decoded_expr)\n",
        "\n",
        "        return decoded_expressions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T09:57:56.06312Z",
          "iopub.status.busy": "2025-04-01T09:57:56.062797Z",
          "iopub.status.idle": "2025-04-01T09:57:56.087634Z",
          "shell.execute_reply": "2025-04-01T09:57:56.086694Z",
          "shell.execute_reply.started": "2025-04-01T09:57:56.063097Z"
        },
        "id": "z_xqktSo1Dc4",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "BOS_IDX, PAD_IDX, EOS_IDX, UNK_IDX, SEP_IDX = 0, 1, 2, 3, 4\n",
        "special_symbols = ['<S>', '<PAD>', '</S>', '<UNK>', '<SEP>']\n",
        "tokenizer = Tokenizer(train_df, 500, 500, special_symbols, UNK_IDX, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T09:57:56.088811Z",
          "iopub.status.busy": "2025-04-01T09:57:56.088534Z",
          "iopub.status.idle": "2025-04-01T09:57:56.108663Z",
          "shell.execute_reply": "2025-04-01T09:57:56.107945Z",
          "shell.execute_reply.started": "2025-04-01T09:57:56.088778Z"
        },
        "id": "4Q97E5981Dc5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def normalize_indices(tokenizer, expressions, index_token_pool_size=50, momentum_token_pool_size=50):\n",
        "    # Function to replace indices with a new set of tokens for each expression\n",
        "    def replace_indices(token_list, index_map):\n",
        "        new_index = (f\"_{i}\" for i in range(index_token_pool_size))  # Local generator for new indices\n",
        "        new_tokens = []\n",
        "        for token in token_list:\n",
        "            if \"INDEX_\" in token:\n",
        "                if token not in index_map:\n",
        "                    try:\n",
        "                        index_map[token] = token.rsplit('_',1)[0] + next(new_index)\n",
        "                    except StopIteration:\n",
        "                        # Handle the case where no more indices are available\n",
        "                        raise ValueError(\"Ran out of unique indices, increase token_pool_size\")\n",
        "                new_tokens.append(index_map[token])\n",
        "            else:\n",
        "                new_tokens.append(token)\n",
        "        return new_tokens\n",
        "\n",
        "    def replace_momenta(token_list, index_map):\n",
        "        new_index = (f\"MOMENTUM_{i}\" for i in range(momentum_token_pool_size))  # Local generator for new indices\n",
        "        new_tokens = []\n",
        "        for token in token_list:\n",
        "            if \"MOMENTUM_\" in token:\n",
        "                if token not in index_map:\n",
        "                    try:\n",
        "                        index_map[token] = next(new_index)\n",
        "                    except StopIteration:\n",
        "                        # Handle the case where no more indices are available\n",
        "                        raise ValueError(\"Ran out of unique indices, increase momentum_token_pool_size\")\n",
        "                new_tokens.append(index_map[token])\n",
        "            else:\n",
        "                new_tokens.append(token)\n",
        "        return new_tokens\n",
        "\n",
        "    normalized_expressions = []\n",
        "    # Replace indices in each expression randomly\n",
        "    for expr in tqdm(expressions,desc=\"Normalizing..\"):\n",
        "        toks = tokenizer.src_tokenize(expr,42)\n",
        "        normalized_expressions.append(replace_momenta(replace_indices(toks, {}), {}))\n",
        "\n",
        "    return normalized_expressions\n",
        "\n",
        "\n",
        "def aug_data(df):\n",
        "    # Extract columns\n",
        "    amps = df['Amplitude']\n",
        "    sqamps = df['Squared Amplitude']\n",
        "\n",
        "    # Data augmentation\n",
        "    n_samples = 1 #args.n_samples\n",
        "    aug_amps = []\n",
        "\n",
        "    for amp in tqdm(amps, desc='processing'):\n",
        "        random_seed = [random.randint(1, 1000) for _ in range(n_samples)]\n",
        "        for seed in random_seed:\n",
        "            aug_amps.append(tokenizer.src_replace(amp, seed))\n",
        "    aug_sqamps = [sqamp for sqamp in sqamps for _ in range(n_samples)]\n",
        "\n",
        "    if True:\n",
        "        normal_amps = normalize_indices(tokenizer, aug_amps, 500, 500)\n",
        "        aug_amps = []\n",
        "        for amp in normal_amps:\n",
        "            aug_amps.append(\"\".join(amp))\n",
        "\n",
        "    # Create augmented DataFrame\n",
        "    df_aug = pd.DataFrame({\"Amplitude\": aug_amps, \"Squared Amplitude\": aug_sqamps})\n",
        "\n",
        "    return df_aug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T09:57:56.109815Z",
          "iopub.status.busy": "2025-04-01T09:57:56.109529Z",
          "iopub.status.idle": "2025-04-01T09:58:05.027402Z",
          "shell.execute_reply": "2025-04-01T09:58:05.02625Z",
          "shell.execute_reply.started": "2025-04-01T09:57:56.109778Z"
        },
        "id": "muJMoGsT1Dc5",
        "outputId": "42c093d7-ca0a-4450-e902-338f9822c22b",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing: 100%|██████████| 12441/12441 [00:06<00:00, 1857.82it/s]\n",
            "Normalizing..: 100%|██████████| 12441/12441 [00:02<00:00, 5818.70it/s]\n"
          ]
        }
      ],
      "source": [
        "train_df_aug = aug_data(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T09:58:05.028688Z",
          "iopub.status.busy": "2025-04-01T09:58:05.028431Z",
          "iopub.status.idle": "2025-04-01T09:58:06.101386Z",
          "shell.execute_reply": "2025-04-01T09:58:06.100471Z",
          "shell.execute_reply.started": "2025-04-01T09:58:05.028659Z"
        },
        "id": "sjbZsg8X1Dc5",
        "outputId": "25ffcaa9-7edc-4151-8868-a5ab07ddd3fb",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing: 100%|██████████| 1555/1555 [00:00<00:00, 1944.51it/s]\n",
            "Normalizing..: 100%|██████████| 1555/1555 [00:00<00:00, 6083.72it/s]\n"
          ]
        }
      ],
      "source": [
        "val_df_aug = aug_data(val_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T09:58:06.10239Z",
          "iopub.status.busy": "2025-04-01T09:58:06.102174Z",
          "iopub.status.idle": "2025-04-01T09:58:07.197155Z",
          "shell.execute_reply": "2025-04-01T09:58:07.196302Z",
          "shell.execute_reply.started": "2025-04-01T09:58:06.102371Z"
        },
        "id": "FVgMrarP1Dc5",
        "outputId": "583cbd9f-4a66-4764-e324-fd1112671bf7",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing: 100%|██████████| 1556/1556 [00:00<00:00, 1929.69it/s]\n",
            "Normalizing..: 100%|██████████| 1556/1556 [00:00<00:00, 5764.13it/s]\n"
          ]
        }
      ],
      "source": [
        "test_df_aug = aug_data(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T09:58:07.19858Z",
          "iopub.status.busy": "2025-04-01T09:58:07.198259Z",
          "iopub.status.idle": "2025-04-01T09:58:07.201658Z",
          "shell.execute_reply": "2025-04-01T09:58:07.201045Z",
          "shell.execute_reply.started": "2025-04-01T09:58:07.198547Z"
        },
        "id": "M70ojk7f1Dc5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# train_df_aug.to_csv('train_df_aug.csv')\n",
        "# val_df_aug.to_csv('val_df_aug.csv')\n",
        "# test_df_aug.to_csv('test_df_aug.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T09:58:07.202716Z",
          "iopub.status.busy": "2025-04-01T09:58:07.202439Z",
          "iopub.status.idle": "2025-04-01T09:58:07.219199Z",
          "shell.execute_reply": "2025-04-01T09:58:07.218455Z",
          "shell.execute_reply.started": "2025-04-01T09:58:07.202684Z"
        },
        "id": "Js9VoYn31Dc5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "tokenizer2 = Tokenizer(train_df_aug, 500, 500, special_symbols, UNK_IDX, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T09:58:07.220269Z",
          "iopub.status.busy": "2025-04-01T09:58:07.220046Z",
          "iopub.status.idle": "2025-04-01T09:58:08.826661Z",
          "shell.execute_reply": "2025-04-01T09:58:08.825949Z",
          "shell.execute_reply.started": "2025-04-01T09:58:07.220251Z"
        },
        "id": "2VYsTOiy1Dc5",
        "outputId": "41b91675-dd44-4044-ce7f-0cf6abc54b12",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing source vocab: 100%|██████████| 12441/12441 [00:01<00:00, 7834.75it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "459"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "src_vocab2 = tokenizer2.build_src_vocab(42)\n",
        "len(src_vocab2.itos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T09:58:08.827766Z",
          "iopub.status.busy": "2025-04-01T09:58:08.827446Z",
          "iopub.status.idle": "2025-04-01T09:58:10.384824Z",
          "shell.execute_reply": "2025-04-01T09:58:10.384093Z",
          "shell.execute_reply.started": "2025-04-01T09:58:08.827734Z"
        },
        "id": "CUo6p4uq1Dc5",
        "outputId": "092f481e-31df-4e71-e5f1-0744db135f25",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing target vocab: 100%|██████████| 12441/12441 [00:01<00:00, 8034.43it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "59"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tgt_vocab2 = tokenizer2.build_tgt_vocab()\n",
        "len(tgt_vocab2.itos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T09:58:10.388337Z",
          "iopub.status.busy": "2025-04-01T09:58:10.388111Z",
          "iopub.status.idle": "2025-04-01T09:58:10.39306Z",
          "shell.execute_reply": "2025-04-01T09:58:10.392322Z",
          "shell.execute_reply.started": "2025-04-01T09:58:10.388319Z"
        },
        "id": "t0_w2JFY1Dc5",
        "outputId": "b72e7ccb-3335-48f7-f037-e35971685a3b",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<S>', '<PAD>', '</S>', '<UNK>', '<SEP>']"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer2.special_symbols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T09:58:10.394414Z",
          "iopub.status.busy": "2025-04-01T09:58:10.394179Z",
          "iopub.status.idle": "2025-04-01T09:58:10.409751Z",
          "shell.execute_reply": "2025-04-01T09:58:10.408882Z",
          "shell.execute_reply.started": "2025-04-01T09:58:10.394393Z"
        },
        "id": "LrbxUZYD1Dc5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# tokenizer2.batch_decode([[2,10,21]],tgt_vocab2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PDXzgAI1Dc5"
      },
      "source": [
        "# dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T09:58:10.411034Z",
          "iopub.status.busy": "2025-04-01T09:58:10.410694Z",
          "iopub.status.idle": "2025-04-01T09:58:10.425842Z",
          "shell.execute_reply": "2025-04-01T09:58:10.424951Z",
          "shell.execute_reply.started": "2025-04-01T09:58:10.410996Z"
        },
        "id": "nUiSu34F1Dc5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#from fn_utils import causal_mask\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "#from ..constants import EOS_IDX,BOS_IDX,PAD_IDX\n",
        "def causal_mask(size):\n",
        "    \"\"\"Create a causal mask for a sequence of given size.\"\"\"\n",
        "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).int()\n",
        "    return mask == 0\n",
        "\n",
        "class Data(Dataset):\n",
        "    \"\"\"\n",
        "    Custom PyTorch dataset for handling data.\n",
        "\n",
        "    Args:\n",
        "        df (DataFrame): DataFrame containing data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, df, tokenizer, config, src_vocab, tgt_vocab):\n",
        "        super(Data, self).__init__()\n",
        "        self.tgt_vals = df['Squared Amplitude']\n",
        "        self.src_vals = df['Amplitude']\n",
        "        self.tgt_tokenize = tokenizer.tgt_tokenize\n",
        "        self.src_tokenize = tokenizer.src_tokenize\n",
        "        self.bos_token = torch.tensor([BOS_IDX], dtype=torch.int64)\n",
        "        self.eos_token = torch.tensor([EOS_IDX], dtype=torch.int64)\n",
        "        self.pad_token = torch.tensor([PAD_IDX], dtype=torch.int64)\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "        self.config = config\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Get the length of the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: Length of the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.src_vals)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Get an item from the dataset at the specified index.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the item.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Tuple containing source and target tensors.\n",
        "        \"\"\"\n",
        "        # print(f'index: {idx}')\n",
        "        src_tokenized = self.src_tokenize(self.src_vals[idx],self.config.seed)\n",
        "        tgt_tokenized = self.tgt_tokenize(self.tgt_vals[idx])\n",
        "        src_ids = self.src_vocab.forward(src_tokenized)\n",
        "        tgt_ids = self.tgt_vocab.forward(tgt_tokenized)\n",
        "\n",
        "        enc_num_padding_tokens = self.config.src_max_len - len(src_ids) - 2\n",
        "        dec_num_padding_tokens = self.config.tgt_max_len - len(tgt_ids) - 1\n",
        "        # print(f'src_ids: {len(src_ids)} tgt_ids:  {len(tgt_ids)} enc_num: {enc_num_padding_tokens} dec_num: {dec_num_padding_tokens} \\n' )\n",
        "        if self.config.truncate:\n",
        "            if enc_num_padding_tokens < 0:\n",
        "                src_ids = src_ids[:self.config.src_max_len-2]\n",
        "            if dec_num_padding_tokens < 0:\n",
        "                tgt_ids = tgt_ids[:self.config.tgt_max_len-1]\n",
        "        else:\n",
        "            if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
        "                raise ValueError(\"Sentence is too long\")\n",
        "        src_tensor = torch.cat(\n",
        "            [\n",
        "                self.bos_token,\n",
        "                torch.tensor(src_ids, dtype=torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token] *\n",
        "                             enc_num_padding_tokens, dtype=torch.int64),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        tgt_tensor = torch.cat(\n",
        "            [\n",
        "                self.bos_token,\n",
        "                torch.tensor(tgt_ids, dtype=torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token] *\n",
        "                             dec_num_padding_tokens, dtype=torch.int64),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        label = torch.cat(\n",
        "            [\n",
        "                torch.tensor(tgt_ids, dtype=torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        src_mask = (src_tensor != self.pad_token).unsqueeze(0).unsqueeze(0).int() # (1, 1, seq_len)\n",
        "        tgt_mask = (tgt_tensor != self.pad_token).unsqueeze(0).int() & causal_mask(tgt_tensor.size(0)) # (1, seq_len) & (1, seq_len, seq_len),\n",
        "\n",
        "        return src_tensor, tgt_tensor, label, src_mask, tgt_mask#, len(src_ids), len(tgt_ids)\n",
        "        # return {'input_ids':src_tensor,'labels':label,'src_attn_mask':src_mask,'tgt_attn_mask':tgt_mask}\n",
        "\n",
        "    @staticmethod\n",
        "    def get_data(df_train, df_test, df_valid, config, tokenizer, src_vocab,tgt_vocab):\n",
        "        \"\"\"\n",
        "        Create datasets (train, test, and valid)\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary containing train, test, and valid datasets.\n",
        "        \"\"\"\n",
        "        train = Data(df_train, tokenizer, config,src_vocab,tgt_vocab)\n",
        "        test = Data(df_test, tokenizer, config,src_vocab,tgt_vocab)\n",
        "        valid = Data(df_valid, tokenizer, config,src_vocab,tgt_vocab)\n",
        "\n",
        "        return {'train': train, 'test': test, 'valid': valid}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T09:58:10.427228Z",
          "iopub.status.busy": "2025-04-01T09:58:10.426867Z",
          "iopub.status.idle": "2025-04-01T09:58:10.445128Z",
          "shell.execute_reply": "2025-04-01T09:58:10.44435Z",
          "shell.execute_reply.started": "2025-04-01T09:58:10.427196Z"
        },
        "id": "E6a7Oecm1Dc5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class d_config:\n",
        "    src_max_len = 300\n",
        "    tgt_max_len = 350\n",
        "    truncate = False\n",
        "    seed = 42\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T09:58:10.446476Z",
          "iopub.status.busy": "2025-04-01T09:58:10.446179Z",
          "iopub.status.idle": "2025-04-01T09:58:10.475728Z",
          "shell.execute_reply": "2025-04-01T09:58:10.474536Z",
          "shell.execute_reply.started": "2025-04-01T09:58:10.446443Z"
        },
        "id": "PRXgZZSi1Dc5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "dataset = Data.get_data(train_df_aug, test_df_aug, val_df_aug, d_config, tokenizer2, src_vocab2, tgt_vocab2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CwWeNHS1Dc5",
        "trusted": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-qUrwuW1Dc5"
      },
      "source": [
        "# Helpers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T12:57:28.834606Z",
          "iopub.status.busy": "2025-02-21T12:57:28.83427Z",
          "iopub.status.idle": "2025-02-21T12:57:28.838211Z",
          "shell.execute_reply": "2025-02-21T12:57:28.83726Z",
          "shell.execute_reply.started": "2025-02-21T12:57:28.834572Z"
        },
        "id": "XTIj3nJi1Dc5"
      },
      "source": [
        "## utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEqOhPYk1Dc8",
        "trusted": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBpaEhly1Dc8"
      },
      "source": [
        "## flash_cross_attention.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T09:58:10.476815Z",
          "iopub.status.busy": "2025-04-01T09:58:10.476586Z",
          "iopub.status.idle": "2025-04-01T09:58:15.690315Z",
          "shell.execute_reply": "2025-04-01T09:58:15.689454Z",
          "shell.execute_reply.started": "2025-04-01T09:58:10.476794Z"
        },
        "id": "gtpjD5Im1Dc9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from mamba_ssm.utils.generation import InferenceParams\n",
        "from flash_attn.modules.mha import MHA\n",
        "from x_transformers.x_transformers import Attention\n",
        "from x_transformers.x_transformers import RelativePositionBias as xRelativePositionBias\n",
        "\n",
        "\n",
        "from math import sqrt\n",
        "\n",
        "try:\n",
        "    from mamba_ssm.ops.triton.layer_norm import RMSNorm, layer_norm_fn, rms_norm_fn\n",
        "except ImportError:\n",
        "    RMSNorm, layer_norm_fn, rms_norm_fn = None, None, None\n",
        "\n",
        "\n",
        "class FlashCrossAttentionWrapper(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        layer_idx=0,\n",
        "        d_model: int = 512,\n",
        "        n_head: int = 8,\n",
        "        rms_norm: bool = True,\n",
        "        norm_epsilon: float = 1e-5,\n",
        "        residual_in_fp32=True,\n",
        "        fused_add_norm=True,\n",
        "        dropout=0.1,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layer_idx = layer_idx\n",
        "        self.residual_in_fp32 = residual_in_fp32\n",
        "        self.fused_add_norm = fused_add_norm\n",
        "\n",
        "        self.attention = Attention(\n",
        "            dim=d_model,\n",
        "            dim_head=d_model // n_head,\n",
        "            heads=n_head,\n",
        "            causal=False,  # x attention should not be causal\n",
        "            dropout=dropout,\n",
        "            flash=True,\n",
        "        )\n",
        "\n",
        "        self.rel_pos = xRelativePositionBias(\n",
        "            scale= (1 / d_model // n_head) ** 0.5,\n",
        "            heads=n_head,\n",
        "        )\n",
        "\n",
        "        # self.attention = MHA(\n",
        "        #     embed_dim=d_model,\n",
        "        #     num_heads=n_head,\n",
        "        #     cross_attn=True,\n",
        "        #     # causal=True,\n",
        "        #     dropout=dropout,\n",
        "        #     layer_idx=layer_idx,\n",
        "        #     # use_flash_attn=True, # not supported if cross_attn\n",
        "        # )\n",
        "\n",
        "        norm_cls = partial(\n",
        "            nn.LayerNorm if not rms_norm else RMSNorm,\n",
        "            eps=norm_epsilon,  # **factory_kwargs\n",
        "        )\n",
        "\n",
        "        self.norm = norm_cls(d_model)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        context: Optional[torch.Tensor] = None,\n",
        "        context_mask: Optional[torch.Tensor] = None,\n",
        "        residual: Optional[torch.Tensor] = None,\n",
        "        mask: Optional[torch.Tensor] = None,\n",
        "        inference_params: InferenceParams = None,\n",
        "    ):\n",
        "        r\"\"\"Pass the input through the encoder layer.\n",
        "\n",
        "        Args:\n",
        "            hidden_states: the sequence to the encoder layer (required).\n",
        "            residual: hidden_states = Mixer(LN(residual))\n",
        "        \"\"\"\n",
        "\n",
        "        if not self.fused_add_norm:\n",
        "            residual = (\n",
        "                (hidden_states + residual) if residual is not None else hidden_states\n",
        "            )\n",
        "            hidden_states = self.norm(residual.to(dtype=self.norm.weight.dtype))\n",
        "            if self.residual_in_fp32:\n",
        "                residual = residual.to(torch.float32)\n",
        "        else:\n",
        "            fused_add_norm_fn = (\n",
        "                rms_norm_fn if isinstance(self.norm, RMSNorm) else layer_norm_fn\n",
        "            )\n",
        "            hidden_states, residual = fused_add_norm_fn(\n",
        "                hidden_states,\n",
        "                self.norm.weight,\n",
        "                self.norm.bias,\n",
        "                residual=residual,\n",
        "                prenorm=True,\n",
        "                residual_in_fp32=self.residual_in_fp32,\n",
        "                eps=self.norm.eps,\n",
        "            )\n",
        "\n",
        "        cache = None\n",
        "\n",
        "        if inference_params is not None and inference_params.seqlen_offset > 0:\n",
        "            cache = inference_params.key_value_memory_dict[self.layer_idx]\n",
        "            cache.seqlen_offset = inference_params.seqlen_offset\n",
        "\n",
        "        hidden_states, cache = self.attention.forward(\n",
        "            x=hidden_states,\n",
        "            mask=mask,\n",
        "            context=context,\n",
        "            context_mask=context_mask,\n",
        "            # rel_pos=self.rel_pos,\n",
        "            return_intermediates=True,\n",
        "            cache=cache,\n",
        "        )\n",
        "\n",
        "        if inference_params is not None:\n",
        "            inference_params.key_value_memory_dict[self.layer_idx] = cache\n",
        "            # hidden_states = (\n",
        "            #     hidden_states[:, -1:, :]\n",
        "            #     if inference_params.seqlen_offset > 0\n",
        "            #     else hidden_states\n",
        "            # )\n",
        "\n",
        "        return hidden_states, residual\n",
        "\n",
        "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5AxgNT71Dc9"
      },
      "source": [
        "## cross_attention.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T09:58:15.691659Z",
          "iopub.status.busy": "2025-04-01T09:58:15.691236Z",
          "iopub.status.idle": "2025-04-01T09:58:15.710882Z",
          "shell.execute_reply": "2025-04-01T09:58:15.71Z",
          "shell.execute_reply.started": "2025-04-01T09:58:15.691636Z"
        },
        "id": "ajjtUdVm1Dc9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from mamba_ssm.utils.generation import InferenceParams\n",
        "from torchscale.architecture.config import DecoderConfig\n",
        "from torchscale.component.multihead_attention import MultiheadAttention\n",
        "from torchscale.component.relative_position_bias import RelativePositionBias\n",
        "\n",
        "try:\n",
        "    from mamba_ssm.ops.triton.layer_norm import RMSNorm, layer_norm_fn, rms_norm_fn\n",
        "except ImportError:\n",
        "    RMSNorm, layer_norm_fn, rms_norm_fn = None, None, None\n",
        "\n",
        "\n",
        "class CrossAttentionWrapper(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        layer_idx=0,\n",
        "        d_model: int = 512,\n",
        "        n_head: int = 8,\n",
        "        rms_norm: bool = True,\n",
        "        norm_epsilon: float = 1e-5,\n",
        "        residual_in_fp32=True,\n",
        "        fused_add_norm=True,\n",
        "        attention_dropout=0,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layer_idx = layer_idx\n",
        "        self.residual_in_fp32 = residual_in_fp32\n",
        "        self.fused_add_norm = fused_add_norm\n",
        "\n",
        "        args = DecoderConfig(\n",
        "            rel_pos_buckets=32, max_rel_pos=128, decoder_attention_heads=n_head\n",
        "        )\n",
        "\n",
        "        self.attention = MultiheadAttention(\n",
        "            args,\n",
        "            d_model,\n",
        "            num_heads=n_head,\n",
        "            dropout=attention_dropout,\n",
        "            self_attention=False,\n",
        "            encoder_decoder_attention=True,\n",
        "            subln=args.subln,\n",
        "        )\n",
        "\n",
        "        self.cross_attn_relative_position = RelativePositionBias(\n",
        "            num_buckets=args.rel_pos_buckets,\n",
        "            max_distance=args.max_rel_pos,\n",
        "            n_heads=args.decoder_attention_heads,\n",
        "        )\n",
        "\n",
        "        norm_cls = partial(\n",
        "            nn.LayerNorm if not rms_norm else RMSNorm,\n",
        "            eps=norm_epsilon,  # **factory_kwargs\n",
        "        )\n",
        "\n",
        "        self.norm = norm_cls(d_model)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        context: Optional[torch.Tensor] = None,\n",
        "        context_mask: Optional[torch.Tensor] = None,\n",
        "        residual: Optional[torch.Tensor] = None,\n",
        "        mask: Optional[torch.Tensor] = None,\n",
        "        inference_params: InferenceParams = None,\n",
        "    ):\n",
        "        r\"\"\"Pass the input through the encoder layer.\n",
        "\n",
        "        Args:\n",
        "            hidden_states: the sequence to the encoder layer (required).\n",
        "            residual: hidden_states = Mixer(LN(residual))\n",
        "        \"\"\"\n",
        "        if not self.fused_add_norm:\n",
        "            residual = (\n",
        "                (hidden_states + residual) if residual is not None else hidden_states\n",
        "            )\n",
        "            hidden_states = self.norm(residual.to(dtype=self.norm.weight.dtype))\n",
        "            if self.residual_in_fp32:\n",
        "                residual = residual.to(torch.float32)\n",
        "        else:\n",
        "            fused_add_norm_fn = (\n",
        "                rms_norm_fn if isinstance(self.norm, RMSNorm) else layer_norm_fn\n",
        "            )\n",
        "            hidden_states, residual = fused_add_norm_fn(\n",
        "                hidden_states,\n",
        "                self.norm.weight,\n",
        "                self.norm.bias,\n",
        "                residual=residual,\n",
        "                prenorm=True,\n",
        "                residual_in_fp32=self.residual_in_fp32,\n",
        "                eps=self.norm.eps,\n",
        "            )\n",
        "\n",
        "        bsz, slen, _ = hidden_states.shape\n",
        "\n",
        "        incremental_state = None\n",
        "\n",
        "        if inference_params is not None and inference_params.seqlen_offset == 0:\n",
        "            slen = 1\n",
        "            incremental_state = {}\n",
        "\n",
        "        elif inference_params is not None and inference_params.seqlen_offset > 0:\n",
        "            slen = inference_params.seqlen_offset\n",
        "            incremental_state = inference_params.key_value_memory_dict[self.layer_idx]\n",
        "\n",
        "        cross_attn_rel_pos = self.cross_attn_relative_position(\n",
        "            batch_size=bsz,\n",
        "            qlen=slen,\n",
        "            klen=context.size(1),\n",
        "        )\n",
        "\n",
        "        if inference_params is not None and inference_params.seqlen_offset != 0:\n",
        "            cross_attn_rel_pos = cross_attn_rel_pos[-1:, :, :]\n",
        "\n",
        "        hidden_states, attn = self.attention.forward(\n",
        "            query=hidden_states,\n",
        "            key=context,\n",
        "            value=context,\n",
        "            key_padding_mask=context_mask,\n",
        "            incremental_state=incremental_state,\n",
        "            # rel_pos=cross_attn_rel_pos,  # none in the default torchscale config\n",
        "        )\n",
        "\n",
        "        if inference_params is not None and inference_params.seqlen_offset == 0:\n",
        "            inference_params.key_value_memory_dict[self.layer_idx] = incremental_state\n",
        "\n",
        "        return hidden_states, residual\n",
        "\n",
        "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLpmkgWc1Dc9"
      },
      "source": [
        "## ffn.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T09:58:15.712112Z",
          "iopub.status.busy": "2025-04-01T09:58:15.711836Z",
          "iopub.status.idle": "2025-04-01T09:58:15.749032Z",
          "shell.execute_reply": "2025-04-01T09:58:15.748236Z",
          "shell.execute_reply.started": "2025-04-01T09:58:15.712089Z"
        },
        "id": "ePYzkFH91Dc9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from mamba_ssm.utils.generation import InferenceParams\n",
        "from flash_attn.modules.mlp import GatedMlp\n",
        "\n",
        "try:\n",
        "    from mamba_ssm.ops.triton.layer_norm import RMSNorm, layer_norm_fn, rms_norm_fn\n",
        "except ImportError:\n",
        "    RMSNorm, layer_norm_fn, rms_norm_fn = None, None, None\n",
        "\n",
        "\n",
        "class FeedForwardWrapper(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        layer_idx=0,\n",
        "        d_model: int = 512,\n",
        "        n_head: int = 8,\n",
        "        rms_norm: bool = True,\n",
        "        norm_epsilon: float = 1e-5,\n",
        "        residual_in_fp32=True,\n",
        "        fused_add_norm=True,\n",
        "        attention_dropout=0,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layer_idx = layer_idx\n",
        "        self.residual_in_fp32 = residual_in_fp32\n",
        "        self.fused_add_norm = fused_add_norm\n",
        "\n",
        "        norm_cls = partial(\n",
        "            nn.LayerNorm if not rms_norm else RMSNorm,\n",
        "            eps=norm_epsilon,  # **factory_kwargs\n",
        "        )\n",
        "\n",
        "        self.norm = norm_cls(d_model)\n",
        "\n",
        "        self.mlp = GatedMlp(\n",
        "            in_features=d_model, hidden_features=4 * d_model, activation=F.silu\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        context: Optional[torch.Tensor] = None,\n",
        "        context_mask: Optional[torch.Tensor] = None,\n",
        "        residual: Optional[torch.Tensor] = None,\n",
        "        mask: Optional[torch.Tensor] = None,\n",
        "        inference_params: InferenceParams = None,\n",
        "    ):\n",
        "        r\"\"\"Pass the input through the encoder layer.\n",
        "\n",
        "        Args:\n",
        "            hidden_states: the sequence to the encoder layer (required).\n",
        "            residual: hidden_states = Mixer(LN(residual))\n",
        "        \"\"\"\n",
        "\n",
        "        if not self.fused_add_norm:\n",
        "            residual = (\n",
        "                (hidden_states + residual) if residual is not None else hidden_states\n",
        "            )\n",
        "            hidden_states = self.norm(residual.to(dtype=self.norm.weight.dtype))\n",
        "            if self.residual_in_fp32:\n",
        "                residual = residual.to(torch.float32)\n",
        "        else:\n",
        "            fused_add_norm_fn = (\n",
        "                rms_norm_fn if isinstance(self.norm, RMSNorm) else layer_norm_fn\n",
        "            )\n",
        "            hidden_states, residual = fused_add_norm_fn(\n",
        "                hidden_states,\n",
        "                self.norm.weight,\n",
        "                self.norm.bias,\n",
        "                residual=residual,\n",
        "                prenorm=True,\n",
        "                residual_in_fp32=self.residual_in_fp32,\n",
        "                eps=self.norm.eps,\n",
        "            )\n",
        "\n",
        "        hidden_states = self.mlp.forward(\n",
        "            x=hidden_states,\n",
        "        )\n",
        "\n",
        "        return hidden_states, residual\n",
        "\n",
        "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsZqmEB11Dc9"
      },
      "source": [
        "## mamba.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T09:58:15.750181Z",
          "iopub.status.busy": "2025-04-01T09:58:15.749881Z",
          "iopub.status.idle": "2025-04-01T09:58:15.78851Z",
          "shell.execute_reply": "2025-04-01T09:58:15.787419Z",
          "shell.execute_reply.started": "2025-04-01T09:58:15.750154Z"
        },
        "id": "67HHzTmo1Dc9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from torch import Tensor\n",
        "\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "from functools import partial\n",
        "from mamba_ssm.models.config_mamba import MambaConfig\n",
        "from mamba_ssm.models.mixer_seq_simple import _init_weights\n",
        "from mamba_ssm.utils.generation import InferenceParams\n",
        "from mamba_ssm.modules.mamba_simple import Mamba\n",
        "\n",
        "\n",
        "try:\n",
        "    from mamba_ssm.ops.triton.layer_norm import RMSNorm, layer_norm_fn, rms_norm_fn\n",
        "except ImportError:\n",
        "    RMSNorm, layer_norm_fn, rms_norm_fn = None, None, None\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        mixer_cls,\n",
        "        norm_cls=nn.LayerNorm,\n",
        "        fused_add_norm=False,\n",
        "        residual_in_fp32=False,\n",
        "        layer_idx=None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Simple block wrapping a mixer class with LayerNorm/RMSNorm and residual connection\"\n",
        "\n",
        "        This Block has a slightly different structure compared to a regular\n",
        "        prenorm Transformer block.\n",
        "        The standard block is: LN -> MHA/MLP -> Add.\n",
        "        [Ref: https://arxiv.org/abs/2002.04745]\n",
        "        Here we have: Add -> LN -> Mixer, returning both\n",
        "        the hidden_states (output of the mixer) and the residual.\n",
        "        This is purely for performance reasons, as we can fuse add and LayerNorm.\n",
        "        The residual needs to be provided (except for the very first block).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.layer_idx = layer_idx\n",
        "\n",
        "        self.residual_in_fp32 = residual_in_fp32\n",
        "        self.fused_add_norm = fused_add_norm\n",
        "        self.mixer = mixer_cls(dim)\n",
        "        self.norm = norm_cls(dim)\n",
        "        if self.fused_add_norm:\n",
        "            assert RMSNorm is not None, \"RMSNorm import fails\"\n",
        "            assert isinstance(\n",
        "                self.norm, (nn.LayerNorm, RMSNorm)\n",
        "            ), \"Only LayerNorm and RMSNorm are supported for fused_add_norm\"\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: Tensor,\n",
        "        context=None,\n",
        "        context_mask=None,\n",
        "        residual: Optional[Tensor] = None,\n",
        "        mask: Optional[Tensor] = None,\n",
        "        inference_params: Optional[InferenceParams] = None,\n",
        "    ):\n",
        "        r\"\"\"Pass the input through the encoder layer.\n",
        "\n",
        "        Args:\n",
        "            hidden_states: the sequence to the encoder layer (required).\n",
        "            residual: hidden_states = Mixer(LN(residual))\n",
        "        \"\"\"\n",
        "\n",
        "        if not self.fused_add_norm:\n",
        "            residual = (\n",
        "                (hidden_states + residual) if residual is not None else hidden_states\n",
        "            )\n",
        "            hidden_states = self.norm(residual.to(dtype=self.norm.weight.dtype))\n",
        "            if self.residual_in_fp32:\n",
        "                residual = residual.to(torch.float32)\n",
        "        else:\n",
        "            fused_add_norm_fn = (\n",
        "                rms_norm_fn if isinstance(self.norm, RMSNorm) else layer_norm_fn\n",
        "            )\n",
        "            hidden_states, residual = fused_add_norm_fn(\n",
        "                hidden_states,\n",
        "                self.norm.weight,\n",
        "                self.norm.bias,\n",
        "                residual=residual,\n",
        "                prenorm=True,\n",
        "                residual_in_fp32=self.residual_in_fp32,\n",
        "                eps=self.norm.eps,\n",
        "            )\n",
        "\n",
        "        if inference_params is not None and inference_params.seqlen_offset > 0:\n",
        "            hidden_states = hidden_states[:, -1:, :]\n",
        "\n",
        "        hidden_states = self.mixer(\n",
        "            hidden_states,\n",
        "            # mask=mask,\n",
        "            inference_params=inference_params\n",
        "        )\n",
        "\n",
        "        return hidden_states, residual\n",
        "\n",
        "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
        "        return self.mixer.allocate_inference_cache(\n",
        "            batch_size, max_seqlen, dtype=dtype, **kwargs\n",
        "        )\n",
        "\n",
        "\n",
        "def create_mamba_block(\n",
        "    d_model,\n",
        "    ssm_cfg=None,\n",
        "    norm_epsilon=1e-5,\n",
        "    rms_norm=False,\n",
        "    residual_in_fp32=False,\n",
        "    fused_add_norm=False,\n",
        "    layer_idx=None,\n",
        "    device=None,\n",
        "    dtype=None,\n",
        "    use_fast_path=True,\n",
        "    dropout=0.0, # set in ssm_cfg\n",
        "    **kwargs,\n",
        "):\n",
        "    if ssm_cfg is None:\n",
        "        ssm_cfg = {}\n",
        "    factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "    mixer_cls = partial(\n",
        "        Mamba,\n",
        "        layer_idx=layer_idx,\n",
        "        use_fast_path=use_fast_path,\n",
        "        # **ssm_cfg,\n",
        "        **factory_kwargs,\n",
        "    )\n",
        "    norm_cls = partial(\n",
        "        nn.LayerNorm if not rms_norm else RMSNorm, eps=norm_epsilon, **factory_kwargs\n",
        "    )\n",
        "    block = Block(\n",
        "        d_model,\n",
        "        mixer_cls,\n",
        "        norm_cls=norm_cls,\n",
        "        fused_add_norm=fused_add_norm,\n",
        "        residual_in_fp32=residual_in_fp32,\n",
        "        layer_idx=layer_idx,\n",
        "    )\n",
        "    return block\n",
        "\n",
        "\n",
        "class MixerModel(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        n_layer: int,\n",
        "        vocab_size: int,\n",
        "        ssm_cfg=None,\n",
        "        norm_epsilon: float = 1e-5,\n",
        "        rms_norm: bool = False,\n",
        "        initializer_cfg=None,\n",
        "        fused_add_norm=False,\n",
        "        residual_in_fp32=False,\n",
        "        device=None,\n",
        "        dtype=None,\n",
        "        use_fast_path=True,\n",
        "        layer_dict=None,\n",
        "        **layer_kwargs,\n",
        "    ) -> None:\n",
        "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "        super().__init__()\n",
        "        self.residual_in_fp32 = residual_in_fp32\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model, **factory_kwargs)\n",
        "\n",
        "        # We change the order of residual and layer norm:\n",
        "        # Instead of LN -> Attn / MLP -> Add, we do:\n",
        "        # Add -> LN -> Attn / MLP / Mixer, returning both the residual branch (output of Add) and\n",
        "        # the main branch (output of MLP / Mixer). The model definition is unchanged.\n",
        "        # This is for performance reason: we can fuse add + layer_norm.\n",
        "        self.fused_add_norm = fused_add_norm\n",
        "        if self.fused_add_norm:\n",
        "            if layer_norm_fn is None or rms_norm_fn is None:\n",
        "                raise ImportError(\"Failed to import Triton LayerNorm / RMSNorm kernels\")\n",
        "\n",
        "        layer_kwargs = {\n",
        "            \"d_model\": d_model,\n",
        "            \"ssm_cfg\": ssm_cfg,\n",
        "            \"norm_epsilon\": norm_epsilon,\n",
        "            \"rms_norm\": rms_norm,\n",
        "            \"residual_in_fp32\": residual_in_fp32,\n",
        "            \"fused_add_norm\": fused_add_norm,\n",
        "            \"use_fast_path\": use_fast_path,\n",
        "            \"dropout\": ssm_cfg[\"dropout\"],\n",
        "            **layer_kwargs,\n",
        "            **factory_kwargs,\n",
        "        }\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                (\n",
        "                    create_mamba_block(\n",
        "                        layer_idx=i,\n",
        "                        **layer_kwargs,\n",
        "                    )\n",
        "                    if i not in layer_dict\n",
        "                    else layer_dict[i](layer_idx=i, **layer_kwargs)  # access class\n",
        "                )\n",
        "                for i in range(n_layer)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.norm_f = (nn.LayerNorm if not rms_norm else RMSNorm)(\n",
        "            d_model, eps=norm_epsilon, **factory_kwargs\n",
        "        )\n",
        "\n",
        "        self.apply(\n",
        "            partial(\n",
        "                _init_weights,\n",
        "                n_layer=n_layer,\n",
        "                **(initializer_cfg if initializer_cfg is not None else {}),\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
        "        return {\n",
        "            i: layer.allocate_inference_cache(\n",
        "                batch_size, max_seqlen, dtype=dtype, **kwargs\n",
        "            )\n",
        "            for i, layer in enumerate(self.layers)\n",
        "        }\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids,\n",
        "        context=None,\n",
        "        context_mask=None,\n",
        "        mask=None,\n",
        "        inference_params: InferenceParams = None,\n",
        "    ):\n",
        "        hidden_states = self.embedding.forward(input_ids)\n",
        "        residual = None\n",
        "        for idx, layer in enumerate(self.layers):\n",
        "            # print('layer: ',idx)\n",
        "            hidden_states, residual = layer(\n",
        "                hidden_states,\n",
        "                context=context,\n",
        "                context_mask=context_mask,\n",
        "                residual=residual,\n",
        "                mask=mask,\n",
        "                inference_params=inference_params,\n",
        "            )\n",
        "        if not self.fused_add_norm:\n",
        "            residual = (\n",
        "                (hidden_states + residual) if residual is not None else hidden_states\n",
        "            )\n",
        "            hidden_states = self.norm_f(residual.to(dtype=self.norm_f.weight.dtype))\n",
        "        else:\n",
        "            # Set prenorm=False here since we don't need the residual\n",
        "            fused_add_norm_fn = (\n",
        "                rms_norm_fn if isinstance(self.norm_f, RMSNorm) else layer_norm_fn\n",
        "            )\n",
        "            hidden_states = fused_add_norm_fn(\n",
        "                hidden_states,\n",
        "                self.norm_f.weight,\n",
        "                self.norm_f.bias,\n",
        "                eps=self.norm_f.eps,\n",
        "                residual=residual,\n",
        "                prenorm=False,\n",
        "                residual_in_fp32=self.residual_in_fp32,\n",
        "            )\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class MambaDecoder(\n",
        "    nn.Module,\n",
        "):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        config: MambaConfig,\n",
        "        initializer_cfg=None,\n",
        "        device=None,\n",
        "        dtype=None,\n",
        "        layer_dict=None,\n",
        "        layer_kwargs={},\n",
        "    ) -> None:\n",
        "        self.config = config\n",
        "        d_model = config.d_model\n",
        "        n_layer = config.n_layer\n",
        "        vocab_size = config.vocab_size\n",
        "        ssm_cfg = config.ssm_cfg\n",
        "        rms_norm = config.rms_norm\n",
        "        residual_in_fp32 = config.residual_in_fp32\n",
        "        fused_add_norm = config.fused_add_norm\n",
        "        pad_vocab_size_multiple = config.pad_vocab_size_multiple\n",
        "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "\n",
        "        super().__init__()\n",
        "        # print(vocab_size)\n",
        "        # if vocab_size % pad_vocab_size_multiple != 0:\n",
        "        #     vocab_size += pad_vocab_size_multiple - (\n",
        "        #         vocab_size % pad_vocab_size_multiple\n",
        "        #     )\n",
        "        # print(vocab_size)\n",
        "\n",
        "        self.backbone = MixerModel(\n",
        "            d_model=d_model,\n",
        "            n_layer=n_layer,\n",
        "            vocab_size=vocab_size,\n",
        "            ssm_cfg=ssm_cfg,\n",
        "            rms_norm=rms_norm,\n",
        "            initializer_cfg=initializer_cfg,\n",
        "            fused_add_norm=fused_add_norm,\n",
        "            residual_in_fp32=residual_in_fp32,\n",
        "            # use_fast_path=config.use_fast_path,\n",
        "            layer_dict=layer_dict,\n",
        "            **factory_kwargs,\n",
        "            **layer_kwargs,\n",
        "        )\n",
        "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False, **factory_kwargs)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.apply(\n",
        "            partial(\n",
        "                _init_weights,\n",
        "                n_layer=n_layer,\n",
        "                **(initializer_cfg if initializer_cfg is not None else {}),\n",
        "            )\n",
        "        )\n",
        "        self.tie_weights()\n",
        "\n",
        "    def tie_weights(self):\n",
        "        self.lm_head.weight = self.backbone.embedding.weight\n",
        "\n",
        "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
        "        return self.backbone.allocate_inference_cache(\n",
        "            batch_size, max_seqlen, dtype=dtype, **kwargs\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids,\n",
        "        context=None,\n",
        "        context_mask=None,\n",
        "        attention_mask=None,\n",
        "        position_ids=None,\n",
        "        inference_params=None,\n",
        "        num_last_tokens=0,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        \"position_ids\" is just to be compatible with Transformer generation. We don't use it.\n",
        "        num_last_tokens: if > 0, only return the logits for the last n tokens\n",
        "        \"\"\"\n",
        "        hidden_states = self.backbone(\n",
        "            input_ids,\n",
        "            context=context,\n",
        "            context_mask=context_mask,\n",
        "            mask=attention_mask,\n",
        "            inference_params=inference_params,\n",
        "        )\n",
        "        if num_last_tokens > 0:\n",
        "            hidden_states = hidden_states[:, -num_last_tokens:]\n",
        "        return self.lm_head(hidden_states)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4GDcZ4-1Dc9"
      },
      "source": [
        "# mamba_encdec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T09:58:46.694184Z",
          "iopub.status.busy": "2025-04-01T09:58:46.69383Z",
          "iopub.status.idle": "2025-04-01T09:58:46.722781Z",
          "shell.execute_reply": "2025-04-01T09:58:46.721832Z",
          "shell.execute_reply.started": "2025-04-01T09:58:46.694154Z"
        },
        "id": "xksbm-az1Dc9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from mamba_ssm.models.config_mamba import MambaConfig\n",
        "from mamba_ssm.utils.generation import InferenceParams\n",
        "\n",
        "\n",
        "# from utils.beam_search import BeamSearch\n",
        "import json\n",
        "# from utils.mt.comet import load_comet\n",
        "from transformers.optimization import get_inverse_sqrt_schedule\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "import evaluate\n",
        "\n",
        "# from models.hybrids.helpers.flash_cross_attention import FlashCrossAttentionWrapper\n",
        "# from models.hybrids.helpers.cross_attention import CrossAttentionWrapper\n",
        "# from models.hybrids.helpers.ffn import FeedForwardWrapper\n",
        "# from models.hybrids.helpers.mamba import MambaDecoder, MixerModel\n",
        "\n",
        "\n",
        "class MambaEncDec(pl.LightningModule):\n",
        "    is_encoder_decoder = True\n",
        "    is_concat = False  # FIXME remove\n",
        "    model_name = \"mamba_encdec\"\n",
        "    configs = {\n",
        "        \"default\": {\n",
        "            \"enc_n_layer\": 5,\n",
        "            # mamba config\n",
        "            \"d_model\": 512,\n",
        "            \"n_layer\": 5,\n",
        "            \"rms_norm\": True,\n",
        "            \"fused_add_norm\": True,\n",
        "            \"use_fast_path\": False,\n",
        "            \"learning_rate\": 1e-3,\n",
        "            \"warmup_steps\": 500,\n",
        "            \"weight_decay\": 0.01,\n",
        "            \"devices\": 'cuda:0'\n",
        "        }\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        config=None,\n",
        "        tokenizer=PreTrainedTokenizerFast,\n",
        "        src_vocab_size=459,\n",
        "        tgt_vocab_size=59,\n",
        "        d_model=None,\n",
        "        n_layer=None,\n",
        "        enc_n_layer=None,\n",
        "        rms_norm=None,\n",
        "        fused_add_norm=None,\n",
        "        use_fast_path=None,\n",
        "        dropout=None,\n",
        "        use_padding=None,\n",
        "        precision=\"32-true\",\n",
        "        test_per_sample=True,\n",
        "        test=False,\n",
        "        test_suffix=\"\",\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = MambaConfig(\n",
        "            vocab_size=tgt_vocab_size,\n",
        "            d_model=d_model,\n",
        "            n_layer=n_layer,\n",
        "            rms_norm=rms_norm,\n",
        "            fused_add_norm=fused_add_norm,\n",
        "            # use_fast_path=use_fast_path,\n",
        "            ssm_cfg={\"dropout\": dropout},\n",
        "        )\n",
        "\n",
        "        self.encoder = MixerModel(\n",
        "            vocab_size=src_vocab_size,\n",
        "            d_model=d_model,\n",
        "            n_layer=enc_n_layer,\n",
        "            rms_norm=rms_norm,\n",
        "            fused_add_norm=fused_add_norm,\n",
        "            use_fast_path=use_fast_path,\n",
        "            ssm_cfg={\"dropout\": dropout},\n",
        "            layer_dict={},\n",
        "        )\n",
        "\n",
        "        self.layers = (0, 3, 6, 9, 12, 15)\n",
        "        x_attention_layers = [\n",
        "            (i, FlashCrossAttentionWrapper) for i in (1, 4, 7, 10, 13, 16)\n",
        "        ]\n",
        "        ffn_layers = [(i, FeedForwardWrapper) for i in (2, 5, 8, 11, 14, 17)]\n",
        "\n",
        "        layer_dict = dict(x_attention_layers + ffn_layers)\n",
        "\n",
        "        self.decoder = MambaDecoder(\n",
        "            config=self.config,\n",
        "            layer_dict=layer_dict,\n",
        "            layer_kwargs={\"dropout\":0.1}\n",
        "        )\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.bleu = evaluate.load(\"sacrebleu\")\n",
        "        self.config = config\n",
        "        self.use_padding = use_padding\n",
        "        dtype_map = {\n",
        "            \"bf16-mixed\": torch.bfloat16,\n",
        "            \"16-true\": torch.float16,\n",
        "            \"32-true\": torch.float32,\n",
        "        }\n",
        "        self.precision = dtype_map[precision]\n",
        "\n",
        "        if test:\n",
        "            # self.comet = load_comet()\n",
        "            self.test_per_sample = test_per_sample\n",
        "            self.test_res = []\n",
        "            self.test_suffix = test_suffix\n",
        "\n",
        "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
        "        return self.decoder.allocate_inference_cache(\n",
        "            batch_size, max_seqlen, dtype=dtype, **kwargs\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        context_tokens,\n",
        "        input_ids,\n",
        "        source_attention_mask=None,\n",
        "        target_attention_mask=None,\n",
        "        position_ids=None,\n",
        "        inference_params=None,\n",
        "        num_last_tokens=0,\n",
        "    ):\n",
        "\n",
        "        b, _, _, l = source_attention_mask.shape\n",
        "        source_attention_mask = source_attention_mask.reshape(b,l).to(torch.bool)\n",
        "        target_attention_mask = target_attention_mask.to(torch.bool)\n",
        "\n",
        "        source_vec = self.encoder.forward(\n",
        "            input_ids=context_tokens,\n",
        "            mask=source_attention_mask,\n",
        "        )\n",
        "        # print(source_vec.dtype, source_attention_mask.dtype)\n",
        "        cache = self.allocate_inference_cache(\n",
        "            batch_size=b,\n",
        "            max_seqlen=300 + l + 1,  # source + BOS\n",
        "            dtype=self.precision,\n",
        "        )\n",
        "        inference_params = InferenceParams(\n",
        "            max_seqlen=300 + l + 1,\n",
        "            max_batch_size=b,\n",
        "            key_value_memory_dict=cache,\n",
        "        )\n",
        "\n",
        "        \n",
        "        logits = self.decoder.forward(\n",
        "            input_ids,\n",
        "            context=source_vec,\n",
        "            context_mask=source_attention_mask,\n",
        "            attention_mask=target_attention_mask,\n",
        "            position_ids=position_ids,\n",
        "            inference_params=inference_params,\n",
        "            num_last_tokens=num_last_tokens,\n",
        "        )\n",
        "        return logits\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # source, target, source_attention_mask = (\n",
        "        #     batch[\"input_ids\"],\n",
        "        #     batch[\"labels\"],\n",
        "        #     batch[\"src_attention_mask\"],\n",
        "        # )\n",
        "        source, target, _, source_attention_mask, _ = batch\n",
        "        # source, target, source_attention_mask,\n",
        "\n",
        "        target_attention_mask = (\n",
        "            (target != self.tokenizer.pad_token_id).to(torch.bool).to(source.device)\n",
        "        )\n",
        "        # print(source.type(), source_attention_mask.type())\n",
        "\n",
        "        lm_logits = self.forward(\n",
        "            context_tokens=source,\n",
        "            source_attention_mask=source_attention_mask,\n",
        "            target_attention_mask=target_attention_mask,\n",
        "            input_ids=target,\n",
        "        )\n",
        "\n",
        "        logits = lm_logits[:, :-1].contiguous()\n",
        "        labels = target[:, 1:].contiguous()\n",
        "\n",
        "        loss = F.cross_entropy(\n",
        "            logits.view(-1, logits.size(-1)),\n",
        "            labels.view(-1),\n",
        "            ignore_index=self.tokenizer.pad_token_id,\n",
        "        )\n",
        "        # self.log(\"train_loss\", loss, sync_dist=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        # src_tokens, labels, source_attention_mask = (\n",
        "        #     batch[\"input_ids\"],\n",
        "        #     batch[\"labels\"],\n",
        "        #     batch[\"src_attention_mask\"],\n",
        "        # )\n",
        "        src_tokens, target, _, source_attention_mask, _ = batch\n",
        "\n",
        "        batch_size, seq_len = src_tokens.shape\n",
        "        max_length = 351\n",
        "\n",
        "        target_attention_mask = (\n",
        "            (target != self.tokenizer.pad_token_id).to(torch.bool).to(src_tokens.device)\n",
        "        )\n",
        "\n",
        "        lm_logits = self.forward(\n",
        "            context_tokens=src_tokens,\n",
        "            source_attention_mask=source_attention_mask,\n",
        "            target_attention_mask=target_attention_mask,\n",
        "            input_ids=target,\n",
        "        )\n",
        "\n",
        "        logits = lm_logits[:, :-1].contiguous()\n",
        "        labels = target[:, 1:].contiguous()\n",
        "\n",
        "        loss = F.cross_entropy(\n",
        "            logits.view(-1, logits.size(-1)),\n",
        "            labels.view(-1),\n",
        "            ignore_index=self.tokenizer.pad_token_id,\n",
        "        )\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        \"\"\"beam search with parallel formulation\"\"\"\n",
        "        # num_beams = 1\n",
        "\n",
        "        # # source_tokens, labels, source_attention_mask = (\n",
        "        # #     batch[\"input_ids\"],\n",
        "        # #     batch[\"labels\"],\n",
        "        # #     batch[\"attention_mask\"],\n",
        "        # # )\n",
        "        src_tokens, _, labels, source_attention_mask, _ = batch\n",
        "        batch_size, seq_len = src_tokens.shape\n",
        "        max_length = 350\n",
        "        cache = self.allocate_inference_cache(\n",
        "            batch_size=batch_size,\n",
        "            max_seqlen=max_length + seq_len + 1,  # source + BOS\n",
        "            dtype=self.precision,\n",
        "            # dtype = ,\n",
        "        )\n",
        "        inference_params = InferenceParams(\n",
        "            max_seqlen=max_length + seq_len + 1,\n",
        "            max_batch_size=batch_size,\n",
        "            key_value_memory_dict=cache,\n",
        "        )\n",
        "\n",
        "        done = torch.tensor([False] * batch_size).to(src_tokens.device)\n",
        "        preds = (\n",
        "            torch.ones((batch_size, 1), dtype=torch.long).to(src_tokens.device)\n",
        "            * self.tokenizer.bos_token_id\n",
        "        )\n",
        "\n",
        "        source_vec = self.encoder.forward(\n",
        "            input_ids=src_tokens,\n",
        "            mask=source_attention_mask,\n",
        "        )\n",
        "        # print('Source vec:', source_vec)\n",
        "        position_ids = None\n",
        "\n",
        "        for idx in range(labels.size(1)):\n",
        "\n",
        "            if idx > 0:\n",
        "                last_tokens = preds[:, -1:]  # (B, 1)\n",
        "                position_ids = torch.full(\n",
        "                    (batch_size, 1),\n",
        "                    inference_params.seqlen_offset,\n",
        "                    dtype=torch.long,\n",
        "                    device=src_tokens.device,\n",
        "                )\n",
        "            \n",
        "            logits = self.decoder.forward(\n",
        "                input_ids=preds if idx == 0 else last_tokens,\n",
        "                context=source_vec,\n",
        "                position_ids=position_ids,\n",
        "                inference_params=inference_params,\n",
        "                num_last_tokens=1,\n",
        "            )\n",
        "\n",
        "            next_token_logits = logits[:, -1, :]\n",
        "            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
        "            preds = torch.cat((preds, next_token), dim=-1)\n",
        "            inference_params.seqlen_offset += 1\n",
        "            # print(next_token.dtype)\n",
        "            is_eos = next_token == self.tokenizer.eos_token_id\n",
        "            done = done | is_eos.squeeze(-1)\n",
        "\n",
        "            if done.all():\n",
        "                break\n",
        "\n",
        "        # Create a cumulative sum mask where positions after EOS become True\n",
        "        eos_token_id = self.tokenizer.eos_token_id\n",
        "        eos_mask = (preds == eos_token_id).cumsum(dim=1) > 0\n",
        "        preds[eos_mask] = self.tokenizer.pad_token_id\n",
        "\n",
        "        preds = preds.cpu()\n",
        "        labels = labels.cpu()\n",
        "\n",
        "        import gc\n",
        "        \n",
        "        del cache, logits, next_token_logits, inference_params\n",
        "\n",
        "        if position_ids is not None:\n",
        "            position_ids = position_ids.cpu()\n",
        "            del position_ids\n",
        "            # gc.collect()\n",
        "        eos_mask = eos_mask.cpu()\n",
        "        del eos_mask\n",
        "        gc.collect()\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "        return preds, labels #(token_acc, seq_acc)\n",
        "        \n",
        "    def on_test_epoch_end(self):\n",
        "        # if self.test_per_sample:\n",
        "        if False:\n",
        "            source, target = self.config[\"language_pair\"]\n",
        "\n",
        "            with open(\n",
        "                f\"mt/res/{self.config['dataset']}/{self.config['dataset']}-{source}-{target}-{self.model_name}-{self.test_suffix}.json\",\n",
        "                \"w\",\n",
        "            ) as f:\n",
        "                json.dump(self.test_res, f)\n",
        "\n",
        "    def _reorder_cache(self, cache, beam_idx):\n",
        "        for layer_idx in self.layers:\n",
        "            device = cache[layer_idx][0].device\n",
        "            # {0:(conv_state, ssm_state)}\n",
        "            cache[layer_idx] = (\n",
        "                cache[layer_idx][0].index_select(0, beam_idx.to(device)),\n",
        "                cache[layer_idx][1].index_select(0, beam_idx.to(device)),\n",
        "            )\n",
        "        return cache\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            self.parameters(),\n",
        "            lr=self.config[\"learning_rate\"],\n",
        "            weight_decay=self.config[\"weight_decay\"],\n",
        "            fused=True,\n",
        "        )\n",
        "\n",
        "        scheduler = {\n",
        "            \"scheduler\": get_inverse_sqrt_schedule(\n",
        "                optimizer,\n",
        "                num_warmup_steps=self.config[\"warmup_steps\"],\n",
        "            ),\n",
        "            \"interval\": \"step\",\n",
        "        }\n",
        "\n",
        "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "37e3fe8a7eea4bb9874dfbe0584f0fa5"
          ]
        },
        "execution": {
          "iopub.execute_input": "2025-04-01T10:00:11.425901Z",
          "iopub.status.busy": "2025-04-01T10:00:11.425515Z",
          "iopub.status.idle": "2025-04-01T10:00:12.712331Z",
          "shell.execute_reply": "2025-04-01T10:00:12.711591Z",
          "shell.execute_reply.started": "2025-04-01T10:00:11.425875Z"
        },
        "id": "uMQ2Qosz1Dc-",
        "outputId": "30712cf3-cf58-4c6e-bc0c-6fd7824c001e",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "37e3fe8a7eea4bb9874dfbe0584f0fa5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-44-24ab68e13f22>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(best_model_path)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# # best_model_path = '/kaggle/input/ckpt-mamba-e10/model-epoch10-valloss0.00.pt'\n",
        "# # best_model_path = '/kaggle/input/ssm-ep5-ckpt/model-epoch05-valloss0.01.pt'\n",
        "# best_model_path = '/kaggle/input/e4-d9-ssm-ep18/model-epoch18-valloss0.00.pt'\n",
        "model = MambaEncDec(**MambaEncDec.configs['default'],config = MambaEncDec.configs['default'], tokenizer = tokenizer2, src_vocab_size = len(src_vocab2.itos), tgt_vocab_size = len(tgt_vocab2.itos))\n",
        "best_model_path = '/kaggle/input/ssm-best-ckpt/model-epoch18-valloss0.00 (1).pt'\n",
        "checkpoint = torch.load(best_model_path)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARzEuGp71Dc-"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_8OlL4V1Dc-"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fm-4OBrG1Dc-",
        "trusted": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T10:00:36.527582Z",
          "iopub.status.busy": "2025-04-01T10:00:36.527288Z",
          "iopub.status.idle": "2025-04-01T10:00:36.53353Z",
          "shell.execute_reply": "2025-04-01T10:00:36.532617Z",
          "shell.execute_reply.started": "2025-04-01T10:00:36.52756Z"
        },
        "id": "C9qAEPwG1Dc-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 16\n",
        "learning_rate = 1e-3\n",
        "weight_decay = 0.01\n",
        "num_epochs = 30\n",
        "grad_accumulation_steps = 8\n",
        "max_grad_norm = 1.0\n",
        "warmup_steps = 1000\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "# Create dataloaders\n",
        "dataloaders = {\n",
        "    split: DataLoader(\n",
        "        dataset[split],\n",
        "        batch_size=batch_size if split != 'test' else 4,\n",
        "        shuffle=(split == 'train'),\n",
        "        pin_memory=True,\n",
        "        num_workers=4,\n",
        "        # sampler= DistributedSampler(dataset[split])\n",
        "    ) for split in ['train', 'valid', 'test']\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olsU3Nl21Dc-"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T10:00:43.633089Z",
          "iopub.status.busy": "2025-04-01T10:00:43.63276Z",
          "iopub.status.idle": "2025-04-01T10:00:43.640119Z",
          "shell.execute_reply": "2025-04-01T10:00:43.639257Z",
          "shell.execute_reply.started": "2025-04-01T10:00:43.633054Z"
        },
        "id": "zxVFj8eY1Dc-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def calculate_accuracy(model, dataloader, device):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    total_seq = 0\n",
        "    correct_seq = 0\n",
        "    total_tokens = 0\n",
        "    correct_tokens = 0\n",
        "    loss_sum = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in tqdm(enumerate(dataloader), desc=\"Calculating accuracy\",total=len(dataloader)):\n",
        "            if idx>40:\n",
        "                break\n",
        "            # Move batch to device\n",
        "            batch = [x.to(device) for x in batch]\n",
        "            outputs, labels = model.test_step(batch, 0)\n",
        "            outputs = outputs.cpu()[:,1:]\n",
        "            l = outputs.shape[1]\n",
        "            labels = labels.cpu()\n",
        "            labels = labels[:,:l]\n",
        "            labels[labels == 2] = 1\n",
        "            \n",
        "            total_tokens += outputs.numel()\n",
        "            correct_tokens += ((outputs == labels)).sum().item()\n",
        "            \n",
        "            batch_size = labels.size(0)\n",
        "            seq_correct = torch.all((outputs == labels), dim=1)\n",
        "            correct_seq += seq_correct.sum().item()\n",
        "            total_seq += batch_size\n",
        "\n",
        "\n",
        "\n",
        "    # Calculate metrics\n",
        "    token_accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0\n",
        "    sequence_accuracy = correct_seq / total_seq if total_seq > 0 else 0\n",
        "    avg_loss = loss_sum / len(dataloader)\n",
        "\n",
        "    return {\n",
        "        'token_accuracy': token_accuracy,\n",
        "        'sequence_accuracy': sequence_accuracy,\n",
        "        'loss': avg_loss\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Only run a fraction of batches due to slow autoregressive generation of SSMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T10:00:43.865208Z",
          "iopub.status.busy": "2025-04-01T10:00:43.864876Z",
          "iopub.status.idle": "2025-04-01T10:08:55.878756Z",
          "shell.execute_reply": "2025-04-01T10:08:55.877713Z",
          "shell.execute_reply.started": "2025-04-01T10:00:43.865183Z"
        },
        "id": "NE47cNvR1Dc-",
        "outputId": "b3d257c1-578c-42f9-d02d-a513a5a96d3a",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating accuracy:  11%|█         | 41/389 [08:11<1:09:32, 11.99s/it]\n"
          ]
        }
      ],
      "source": [
        "# out = calculate_accuracy(trained_model, dataloaders['test'], device='cuda:1')\n",
        "out = calculate_accuracy(model, dataloaders['test'], device='cuda:1')\n",
        "model = model.cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T10:08:55.880325Z",
          "iopub.status.busy": "2025-04-01T10:08:55.88006Z",
          "iopub.status.idle": "2025-04-01T10:08:55.885258Z",
          "shell.execute_reply": "2025-04-01T10:08:55.884521Z",
          "shell.execute_reply.started": "2025-04-01T10:08:55.880298Z"
        },
        "id": "iIg9qyy51Dc-",
        "outputId": "c5d793af-4082-4fdb-9dfa-b58594e3c78c",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'token_accuracy': 0.946803900325027,\n",
              " 'sequence_accuracy': 0.9146341463414634,\n",
              " 'loss': 0.0}"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ao0HIgZ1Dc_"
      },
      "source": [
        "#####"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "notebook0fe7443390",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 6729715,
          "sourceId": 10837100,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6894068,
          "sourceId": 11064012,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6907654,
          "sourceId": 11082918,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6914386,
          "sourceId": 11092111,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 7013539,
          "sourceId": 11228742,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
